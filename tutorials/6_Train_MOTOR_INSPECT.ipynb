{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train MOTOR\n",
    "\n",
    "This tutorial walks through the various steps to train a MOTOR model.\n",
    "\n",
    "Training MOTOR is a four step process:\n",
    "\n",
    "- Training a tokenizer\n",
    "- Prefitting MOTOR\n",
    "- Preparing batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = '/share/pi/nigam/zphuo/cache_dir'\n",
    "\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_6_INSEPCT'\n",
    "\n",
    "from_pretrained = False\n",
    "num_proc = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515d858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not from_pretrained:\n",
    "    if os.path.exists(TARGET_DIR):\n",
    "        shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "    os.mkdir(TARGET_DIR)\n",
    "    os.mkdir(os.path.join(TARGET_DIR, 'motor_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646f7590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cda688a771451e80d90db2170ed2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd07a06b87c144bfaf58574d49514f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import femr.index\n",
    "import femr.splits\n",
    "\n",
    "# First, we want to split our dataset into train, valid, and test\n",
    "# We do this by calling our split functionality twice\n",
    "\n",
    "# dataset = datasets.Dataset.from_parquet('input/meds/data/*')\n",
    "parquet_folder = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/data_subset/*'\n",
    "dataset = datasets.Dataset.from_parquet(parquet_folder)\n",
    "\n",
    "\n",
    "index = femr.index.PatientIndex(dataset, num_proc=num_proc)\n",
    "main_split = femr.splits.generate_hash_split(index.get_patient_ids(), 97, frac_test=0.15)\n",
    "\n",
    "\n",
    "# Note that we want to save this to the target directory since this is important information\n",
    "\n",
    "main_split.save_to_csv(os.path.join(TARGET_DIR, \"motor_model\", \"main_split.csv\"))\n",
    "\n",
    "train_split = femr.splits.generate_hash_split(main_split.train_patient_ids, 87, frac_test=0.15)\n",
    "\n",
    "# print(train_split.train_patient_ids)\n",
    "# print(train_split.test_patient_ids)\n",
    "\n",
    "main_dataset = main_split.split_dataset(dataset, index)\n",
    "train_dataset = train_split.split_dataset(main_dataset['train'], femr.index.PatientIndex(main_dataset['train'], num_proc=num_proc))\n",
    "\n",
    "# print(train_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89348e34",
   "metadata": {},
   "source": [
    "main_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8360f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import femr.models.tokenizer\n",
    "from femr.models.tokenizer import FEMRTokenizer\n",
    "import pickle\n",
    "\n",
    "# First, we need to train a tokenizer\n",
    "# Note, we need to use a hierarchical tokenizer for MOTOR\n",
    "\n",
    "\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b659cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d570d5fc7824b77aea8a583f602d254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not from_pretrained:\n",
    "    tokenizer = femr.models.tokenizer.train_tokenizer(\n",
    "        main_dataset['train'], vocab_size=128, is_hierarchical=True, num_proc=num_proc, ontology=ontology)\n",
    "\n",
    "    # Save the tokenizer to the same directory as the model\n",
    "    tokenizer.save_pretrained(os.path.join(TARGET_DIR, \"motor_model\"))\n",
    "\n",
    "else:\n",
    "    # load pretrained tokenizer\n",
    "    tokenizer = femr.models.tokenizer.FEMRTokenizer.from_pretrained(os.path.join(TARGET_DIR, \"motor_model\"), ontology=ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b60daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e167b996efdf47feb2d6d3a8340d3957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import femr.models.tasks\n",
    "\n",
    "if 'subset' in parquet_folder:\n",
    "    num_tasks = 39\n",
    "else:\n",
    "    num_tasks = 64\n",
    "\n",
    "# Second, we need to prefit the MOTOR model. This is necessary because piecewise exponential models are unstable without an initial fit\n",
    "\n",
    "motor_task = femr.models.tasks.MOTORTask.fit_pretraining_task_info(\n",
    "    main_dataset['train'], tokenizer, num_tasks=num_tasks, num_bins=4, final_layer_size=32, num_proc=num_proc)\n",
    "\n",
    "\n",
    "# It's recommended to save this with pickle to avoid recomputing since it's an expensive operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df96242a75f54f99a2967e0e7e841826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches 344\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ffcdca1b854446a58b7c8acfb0c693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0acfe4b532c41b88b20631d70dbacb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches 61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071de6c143b44f11a19fb016fcd83c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert batches to pytorch\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import femr.models.processor\n",
    "import femr.models.tasks\n",
    "\n",
    "# Third, we need to create batches. \n",
    "\n",
    "processor = femr.models.processor.FEMRBatchProcessor(tokenizer, motor_task)\n",
    "\n",
    "# We can do this one patient at a time\n",
    "# print(\"Convert a single patient\")\n",
    "# example_batch = processor.collate([processor.convert_patient(train_dataset['train'][0], tensor_type='pt')])\n",
    "\n",
    "print(\"Convert batches\")\n",
    "# But generally we want to convert entire datasets\n",
    "train_batches = processor.convert_dataset(train_dataset, tokens_per_batch=32, num_proc=num_proc)\n",
    "\n",
    "print(\"Convert batches to pytorch\")\n",
    "# Convert our batches to pytorch tensors\n",
    "train_batches.set_format(\"pt\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dcba7ce",
   "metadata": {},
   "source": [
    "train_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 02:32:08.630451: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-03 02:32:13.280784: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/pi/nigam/projects/zphuo/cuda/lib64::/home/zphuo/packages/cuda/lib64:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/bin\n",
      "2024-02-03 02:32:13.281496: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/pi/nigam/projects/zphuo/cuda/lib64::/home/zphuo/packages/cuda/lib64:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/bin\n",
      "2024-02-03 02:32:13.281514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# Finally, given the batches, we can train CLMBR.\n",
    "# We can use huggingface's trainer to do this.\n",
    "\n",
    "transformer_config = femr.models.transformer.FEMRTransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    is_hierarchical=tokenizer.is_hierarchical, \n",
    "    n_layers=2,\n",
    "    hidden_size=64, \n",
    "    intermediate_size=64*2,\n",
    "    n_heads=8,\n",
    ")\n",
    "\n",
    "config = femr.models.transformer.FEMRModelConfig.from_transformer_task_configs(transformer_config, motor_task.get_task_config())\n",
    "\n",
    "model = femr.models.transformer.FEMRModel(config)\n",
    "\n",
    "collator = processor.collate\n",
    "\n",
    "trainer_config = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "\n",
    "    output_dir='tmp_trainer',\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "\n",
    "    eval_steps=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    logging_steps=20,\n",
    "    logging_strategy='steps',\n",
    "\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=processor.collate,\n",
    "    train_dataset=train_batches['train'],\n",
    "    eval_dataset=train_batches['test'],\n",
    "    args=trainer_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1300074. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1300074. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzphuo\u001b[0m (\u001b[33mstanford_som\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/pi/nigam/projects/zphuo/repos/femr/tutorials/wandb/run-20240203_023227-rdr8mlkv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stanford_som/huggingface/runs/rdr8mlkv' target=\"_blank\">autumn-dust-15</a></strong> to <a href='https://wandb.ai/stanford_som/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stanford_som/huggingface' target=\"_blank\">https://wandb.ai/stanford_som/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stanford_som/huggingface/runs/rdr8mlkv' target=\"_blank\">https://wandb.ai/stanford_som/huggingface/runs/rdr8mlkv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3921' max='34400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3917/34400 02:28 < 19:17, 26.33 it/s, Epoch 11.38/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.390400</td>\n",
       "      <td>0.441015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.439944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>0.438925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.437638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.436170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.448200</td>\n",
       "      <td>0.434653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.405800</td>\n",
       "      <td>0.432995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.431454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.423900</td>\n",
       "      <td>0.429454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.430700</td>\n",
       "      <td>0.427943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.456500</td>\n",
       "      <td>0.426147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.453500</td>\n",
       "      <td>0.424367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.441300</td>\n",
       "      <td>0.422158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.420263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.440500</td>\n",
       "      <td>0.418609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.416952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.488100</td>\n",
       "      <td>0.415368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.495400</td>\n",
       "      <td>0.413382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.411683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>0.410127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.408074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.406012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.404151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>0.402531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.400643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.421800</td>\n",
       "      <td>0.399166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.397819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.396396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.394807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.383700</td>\n",
       "      <td>0.393251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.414400</td>\n",
       "      <td>0.391655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>0.390446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.387900</td>\n",
       "      <td>0.388888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.435000</td>\n",
       "      <td>0.387372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.419200</td>\n",
       "      <td>0.385778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.384315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.371600</td>\n",
       "      <td>0.383227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.436000</td>\n",
       "      <td>0.382054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>0.380958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>0.379822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>0.378463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.377333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.376319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.396800</td>\n",
       "      <td>0.375280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.374362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.373420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.419900</td>\n",
       "      <td>0.372389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.371448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.370898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.370168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.374700</td>\n",
       "      <td>0.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>0.368630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.415600</td>\n",
       "      <td>0.367987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.360600</td>\n",
       "      <td>0.367462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.423700</td>\n",
       "      <td>0.367043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.366800</td>\n",
       "      <td>0.366570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.362300</td>\n",
       "      <td>0.366208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.347600</td>\n",
       "      <td>0.365825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.339500</td>\n",
       "      <td>0.365194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.374600</td>\n",
       "      <td>0.364724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.364184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.373400</td>\n",
       "      <td>0.363713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.363187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.362739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.362496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.423000</td>\n",
       "      <td>0.362113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>0.361599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.361184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.360903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.362000</td>\n",
       "      <td>0.360550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.360249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.359977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.396700</td>\n",
       "      <td>0.359708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.353500</td>\n",
       "      <td>0.359486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.359252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.408000</td>\n",
       "      <td>0.358983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.358766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.364000</td>\n",
       "      <td>0.358545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.397600</td>\n",
       "      <td>0.358322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.368900</td>\n",
       "      <td>0.358152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.357958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>0.357767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.376300</td>\n",
       "      <td>0.357610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.376400</td>\n",
       "      <td>0.357469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>0.357317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.389900</td>\n",
       "      <td>0.357211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.357089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.334900</td>\n",
       "      <td>0.356963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.356813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.379600</td>\n",
       "      <td>0.356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.381400</td>\n",
       "      <td>0.356534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.334400</td>\n",
       "      <td>0.356349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.356164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.360700</td>\n",
       "      <td>0.356020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.355964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.413200</td>\n",
       "      <td>0.355873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.403800</td>\n",
       "      <td>0.355758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.355609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.412600</td>\n",
       "      <td>0.355457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.403500</td>\n",
       "      <td>0.355372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>0.355289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>0.355208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.361100</td>\n",
       "      <td>0.355105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>0.355010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.338300</td>\n",
       "      <td>0.354925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>0.354767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.348200</td>\n",
       "      <td>0.354655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.399700</td>\n",
       "      <td>0.354588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.354532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.354432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.354353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.354299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.356300</td>\n",
       "      <td>0.354218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.354130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.411000</td>\n",
       "      <td>0.354074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.403200</td>\n",
       "      <td>0.354011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.326500</td>\n",
       "      <td>0.353947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.353880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.353788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.383500</td>\n",
       "      <td>0.353711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.375400</td>\n",
       "      <td>0.353657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>0.353593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.391700</td>\n",
       "      <td>0.353568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.365700</td>\n",
       "      <td>0.353504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.353423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.375200</td>\n",
       "      <td>0.353337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.348800</td>\n",
       "      <td>0.353244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.353135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.343700</td>\n",
       "      <td>0.353055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.416100</td>\n",
       "      <td>0.353035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.378300</td>\n",
       "      <td>0.353041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.352976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.352954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.395400</td>\n",
       "      <td>0.352909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.352907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>0.352820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.377600</td>\n",
       "      <td>0.352793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.377800</td>\n",
       "      <td>0.352732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.398600</td>\n",
       "      <td>0.352708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.399300</td>\n",
       "      <td>0.352684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.355100</td>\n",
       "      <td>0.352605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.352563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.358700</td>\n",
       "      <td>0.352522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.352473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.352429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.359200</td>\n",
       "      <td>0.352336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.352259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.352219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.377900</td>\n",
       "      <td>0.352176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.352159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.352106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.369000</td>\n",
       "      <td>0.352051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.353800</td>\n",
       "      <td>0.352012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.352022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.390200</td>\n",
       "      <td>0.351963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.351915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.368400</td>\n",
       "      <td>0.351897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.351834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.382100</td>\n",
       "      <td>0.351791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.351779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.363700</td>\n",
       "      <td>0.351777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.351720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.351703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.336300</td>\n",
       "      <td>0.351613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.371200</td>\n",
       "      <td>0.351548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.351510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.384900</td>\n",
       "      <td>0.351471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.351452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.398400</td>\n",
       "      <td>0.351427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.415200</td>\n",
       "      <td>0.351440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.412600</td>\n",
       "      <td>0.351445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.344200</td>\n",
       "      <td>0.351395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.351415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.370500</td>\n",
       "      <td>0.351381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.308200</td>\n",
       "      <td>0.351307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.351252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.366500</td>\n",
       "      <td>0.351191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.351114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.361800</td>\n",
       "      <td>0.351139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.385200</td>\n",
       "      <td>0.351075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>0.409500</td>\n",
       "      <td>0.351072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.351145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.351202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.351197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.351126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.351059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>0.352500</td>\n",
       "      <td>0.350988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.350952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>0.360800</td>\n",
       "      <td>0.350897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.336300</td>\n",
       "      <td>0.350851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>0.351100</td>\n",
       "      <td>0.350794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.350757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.350758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.336300</td>\n",
       "      <td>0.350704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.341800</td>\n",
       "      <td>0.350626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/61 00:00 < 00:00, 128.78 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory tmp_trainer/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory tmp_trainer/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory tmp_trainer/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory tmp_trainer/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory tmp_trainer/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory tmp_trainer/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory tmp_trainer/checkpoint-3500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(os.path.join(TARGET_DIR, 'motor_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear probe\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
