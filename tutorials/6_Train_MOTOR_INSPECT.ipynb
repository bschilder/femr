{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train MOTOR\n",
    "\n",
    "This tutorial walks through the various steps to train a MOTOR model.\n",
    "\n",
    "Training MOTOR is a four step process:\n",
    "\n",
    "- Training a tokenizer\n",
    "- Prefitting MOTOR\n",
    "- Preparing batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = '/share/pi/nigam/projects/zphuo/.cache'\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_6_INSEPCT'\n",
    "\n",
    "from_pretrained = True\n",
    "num_proc = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "515d858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not from_pretrained:\n",
    "    if os.path.exists(TARGET_DIR):\n",
    "        shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "    os.mkdir(TARGET_DIR)\n",
    "    os.mkdir(os.path.join(TARGET_DIR, 'motor_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646f7590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "865c462223094662ab37c538141f27b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e029b5c9948b4f338ca7c01c4fbcc602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import femr.index\n",
    "import femr.splits\n",
    "\n",
    "# First, we want to split our dataset into train, valid, and test\n",
    "# We do this by calling our split functionality twice\n",
    "\n",
    "# dataset = datasets.Dataset.from_parquet('input/meds/data/*')\n",
    "parquet_folder = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/data_subset/*'\n",
    "dataset = datasets.Dataset.from_parquet(parquet_folder)\n",
    "\n",
    "\n",
    "index = femr.index.PatientIndex(dataset, num_proc=num_proc)\n",
    "main_split = femr.splits.generate_hash_split(index.get_patient_ids(), 97, frac_test=0.15)\n",
    "\n",
    "\n",
    "# Note that we want to save this to the target directory since this is important information\n",
    "\n",
    "main_split.save_to_csv(os.path.join(TARGET_DIR, \"motor_model\", \"main_split.csv\"))\n",
    "\n",
    "import pandas as pd\n",
    "label_csv_subset = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/cohort_0.2.0_master_file_anon_subset.csv'\n",
    "label_df = pd.read_csv(label_csv_subset)\n",
    "label_df = label_df[['patient_id', 'split', ]]\n",
    "inspect_split_csv = '/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/main_split.csv'\n",
    "label_df.to_csv(inspect_split_csv, index=False)\n",
    "\n",
    "train_split = femr.splits.generate_hash_split(main_split.train_patient_ids, 87, frac_test=0.15)\n",
    "\n",
    "# print(train_split.train_patient_ids)\n",
    "# print(train_split.test_patient_ids)\n",
    "\n",
    "main_dataset = main_split.split_dataset(dataset, index)\n",
    "train_dataset = train_split.split_dataset(main_dataset['train'], femr.index.PatientIndex(main_dataset['train'], num_proc=num_proc))\n",
    "\n",
    "# print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import femr.models.tokenizer\n",
    "from femr.models.tokenizer import FEMRTokenizer\n",
    "import pickle\n",
    "\n",
    "# First, we need to train a tokenizer\n",
    "# Note, we need to use a hierarchical tokenizer for MOTOR\n",
    "\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b659cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not from_pretrained:\n",
    "    tokenizer = femr.models.tokenizer.train_tokenizer(\n",
    "        main_dataset['train'], vocab_size=128, is_hierarchical=True, num_proc=num_proc, ontology=ontology)\n",
    "\n",
    "    # Save the tokenizer to the same directory as the model\n",
    "    tokenizer.save_pretrained(os.path.join(TARGET_DIR, \"motor_model\"))\n",
    "\n",
    "else:\n",
    "    # load pretrained tokenizer\n",
    "    tokenizer = femr.models.tokenizer.FEMRTokenizer.from_pretrained(os.path.join(TARGET_DIR, \"motor_model\"), ontology=ontology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b60daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e44f03119634e2bb013a95a87c09c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1639 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Couldn't find patient birthdate -- Patient has no birth events (which must be {meds.birth_code}): [{'time': datetime.datetime(2123, 1, 1, 0, 0), 'measurements': [{'code': 'SNOMED/3950001', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': None, 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2123, 1, 1, 23, 59), 'measurements': [{'code': 'Race/2', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': 'Declines to State | Asian', 'unit': None, 'visit_id': None}}, {'code': 'Gender/F', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': '1 | 1', 'unit': None, 'visit_id': None}}, {'code': 'Ethnicity/Not Hispanic', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': 'Non-Hispanic/Non-Latino | Non-Hispanic/Non-Latino', 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2152, 3, 18, 15, 15), 'measurements': [{'code': 'CPT4/87081', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'GC CULTURE SCREEN', 'unit': None, 'visit_id': None}}, {'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': None}}, {'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': None}}, {'code': 'SNOMED/407707008', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'CHLAMYDIA MOLECULAR DETECTION', 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2152, 11, 3, 23, 59), 'measurements': [{'code': 'LOINC/11506-3', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}, {'code': 'LOINC/11506-3', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}]}, {'time': datetime.datetime(2152, 12, 11, 11, 50), 'measurements': [{'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}, {'code': 'SNOMED/117010004', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'URINE CULTURE', 'unit': None, 'visit_id': '28512886.0'}}]}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1377, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3466, in _map_single\n    batch = apply_function_on_filtered_inputs(\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3345, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/share/pi/nigam/projects/zphuo/repos/femr/src/femr/hf_utils.py\", line 6, in _agg_helper\n    result = map_func(*args)\n  File \"/share/pi/nigam/projects/zphuo/repos/femr/src/femr/models/tasks.py\", line 223, in _prefit_motor_map\n    birth = femr.pat_utils.get_patient_birthdate(patient)\n  File \"/share/pi/nigam/projects/zphuo/repos/femr/src/femr/pat_utils.py\", line 12, in get_patient_birthdate\n    raise ValueError(\"Couldn't find patient birthdate -- Patient has no birth events (which must be {meds.birth_code}): \" + str(patient[\"events\"][:5]))\nValueError: Couldn't find patient birthdate -- Patient has no birth events (which must be {meds.birth_code}): [{'time': datetime.datetime(2123, 1, 1, 0, 0), 'measurements': [{'code': 'SNOMED/3950001', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': None, 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2123, 1, 1, 23, 59), 'measurements': [{'code': 'Race/2', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': 'Declines to State | Asian', 'unit': None, 'visit_id': None}}, {'code': 'Gender/F', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': '1 | 1', 'unit': None, 'visit_id': None}}, {'code': 'Ethnicity/Not Hispanic', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': 'Non-Hispanic/Non-Latino | Non-Hispanic/Non-Latino', 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2152, 3, 18, 15, 15), 'measurements': [{'code': 'CPT4/87081', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'GC CULTURE SCREEN', 'unit': None, 'visit_id': None}}, {'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': None}}, {'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': None}}, {'code': 'SNOMED/407707008', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'CHLAMYDIA MOLECULAR DETECTION', 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2152, 11, 3, 23, 59), 'measurements': [{'code': 'LOINC/11506-3', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}, {'code': 'LOINC/11506-3', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}]}, {'time': datetime.datetime(2152, 12, 11, 11, 50), 'measurements': [{'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}, {'code': 'SNOMED/117010004', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'URINE CULTURE', 'unit': None, 'visit_id': '28512886.0'}}]}]\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m     num_tasks \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Second, we need to prefit the MOTOR model. This is necessary because piecewise exponential models are unstable without an initial fit\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m motor_task \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mtasks\u001b[39m.\u001b[39;49mMOTORTask\u001b[39m.\u001b[39;49mfit_pretraining_task_info(\n\u001b[1;32m     11\u001b[0m     main_dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], tokenizer, num_tasks\u001b[39m=\u001b[39;49mnum_tasks, num_bins\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, final_layer_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, num_proc\u001b[39m=\u001b[39;49mnum_proc)\n\u001b[1;32m     14\u001b[0m \u001b[39m# It's recommended to save this with pickle to avoid recomputing since it's an expensive operation\u001b[39;00m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/femr/src/femr/models/tasks.py:278\u001b[0m, in \u001b[0;36mMOTORTask.fit_pretraining_task_info\u001b[0;34m(cls, dataset, tokenizer, num_tasks, num_bins, final_layer_size, num_proc)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(tasks) \u001b[39m==\u001b[39m num_tasks, \u001b[39m\"\u001b[39m\u001b[39mCould not find enough tasks in the provided tokenizer\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 278\u001b[0m length_samples, stats \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39;49mhf_utils\u001b[39m.\u001b[39;49maggregate_over_dataset(\n\u001b[1;32m    279\u001b[0m     dataset,\n\u001b[1;32m    280\u001b[0m     functools\u001b[39m.\u001b[39;49mpartial(_prefit_motor_map, tasks\u001b[39m=\u001b[39;49mtasks, ontology\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49montology),\n\u001b[1;32m    281\u001b[0m     _prefit_motor_agg,\n\u001b[1;32m    282\u001b[0m     \u001b[39m1_000\u001b[39;49m,\n\u001b[1;32m    283\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    284\u001b[0m )\n\u001b[1;32m    286\u001b[0m time_bins \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpercentile(length_samples\u001b[39m.\u001b[39msamples, np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m100\u001b[39m, num_bins \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    287\u001b[0m time_bins[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/femr/src/femr/hf_utils.py:19\u001b[0m, in \u001b[0;36maggregate_over_dataset\u001b[0;34m(dataset, map_func, agg_func, batch_size, num_proc, with_indices)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maggregate_over_dataset\u001b[39m(dataset, map_func, agg_func, batch_size, num_proc, with_indices\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     11\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform an aggregation over a huggingface dataset.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[39m    This logic consists of two parts, map_func and agg_func.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m    agg_func takes those intermediate results and combines them into a final result.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     parts \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m     20\u001b[0m         functools\u001b[39m.\u001b[39;49mpartial(_agg_helper, map_func\u001b[39m=\u001b[39;49mmap_func),\n\u001b[1;32m     21\u001b[0m         batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     22\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     23\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mcolumn_names,\n\u001b[1;32m     24\u001b[0m         num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m     25\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m     26\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     27\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minvalid\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     current \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     \u001b[39mfor\u001b[39;00m stat \u001b[39min\u001b[39;00m parts:\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/arrow_dataset.py:591\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    590\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    592\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    593\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    594\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/arrow_dataset.py:556\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    550\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    551\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    554\u001b[0m }\n\u001b[1;32m    555\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    557\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    558\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/arrow_dataset.py:3181\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3174\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3175\u001b[0m \u001b[39mwith\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   3176\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   3177\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3178\u001b[0m     total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3179\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3180\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3181\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3182\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3183\u001b[0m     ):\n\u001b[1;32m   3184\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3185\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[1;32m   1416\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m         [async_result\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[1;32m   1416\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m         [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/multiprocess/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't find patient birthdate -- Patient has no birth events (which must be {meds.birth_code}): [{'time': datetime.datetime(2123, 1, 1, 0, 0), 'measurements': [{'code': 'SNOMED/3950001', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': None, 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2123, 1, 1, 23, 59), 'measurements': [{'code': 'Race/2', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': 'Declines to State | Asian', 'unit': None, 'visit_id': None}}, {'code': 'Gender/F', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': '1 | 1', 'unit': None, 'visit_id': None}}, {'code': 'Ethnicity/Not Hispanic', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'person', 'source_code': 'Non-Hispanic/Non-Latino | Non-Hispanic/Non-Latino', 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2152, 3, 18, 15, 15), 'measurements': [{'code': 'CPT4/87081', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'GC CULTURE SCREEN', 'unit': None, 'visit_id': None}}, {'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': None}}, {'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': None}}, {'code': 'SNOMED/407707008', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'CHLAMYDIA MOLECULAR DETECTION', 'unit': None, 'visit_id': None}}]}, {'time': datetime.datetime(2152, 11, 3, 23, 59), 'measurements': [{'code': 'LOINC/11506-3', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}, {'code': 'LOINC/11506-3', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}]}, {'time': datetime.datetime(2152, 12, 11, 11, 50), 'measurements': [{'code': 'LOINC/28570-0', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'note', 'source_code': None, 'unit': None, 'visit_id': '28512886.0'}}, {'code': 'SNOMED/117010004', 'text_value': None, 'numeric_value': None, 'datetime_value': None, 'metadata': {'end': None, 'omop_table': 'measurement', 'source_code': 'URINE CULTURE', 'unit': None, 'visit_id': '28512886.0'}}]}]"
     ]
    }
   ],
   "source": [
    "import femr.models.tasks\n",
    "\n",
    "if 'subset' in parquet_folder:\n",
    "    num_tasks = 39\n",
    "else:\n",
    "    num_tasks = 64\n",
    "\n",
    "# Second, we need to prefit the MOTOR model. This is necessary because piecewise exponential models are unstable without an initial fit\n",
    "\n",
    "motor_task = femr.models.tasks.MOTORTask.fit_pretraining_task_info(\n",
    "    main_dataset['train'], tokenizer, num_tasks=num_tasks, num_bins=4, final_layer_size=32, num_proc=num_proc)\n",
    "\n",
    "\n",
    "# It's recommended to save this with pickle to avoid recomputing since it's an expensive operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.models.processor\n",
    "import femr.models.tasks\n",
    "\n",
    "# Third, we need to create batches. \n",
    "\n",
    "processor = femr.models.processor.FEMRBatchProcessor(tokenizer, motor_task)\n",
    "\n",
    "# We can do this one patient at a time\n",
    "print(\"Convert a single patient\")\n",
    "example_batch = processor.collate([processor.convert_patient(train_dataset['train'][0], tensor_type='pt')])\n",
    "\n",
    "# print(\"Convert batches\")\n",
    "# # But generally we want to convert entire datasets\n",
    "# train_batches = processor.convert_dataset(train_dataset, tokens_per_batch=32, num_proc=num_proc, min_samples_per_batch=1)\n",
    "# print(\"Convert batches to pytorch\")\n",
    "# # Convert our batches to pytorch tensors\n",
    "# train_batches.set_format(\"pt\")\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "# Finally, given the batches, we can train CLMBR.\n",
    "# We can use huggingface's trainer to do this.\n",
    "\n",
    "transformer_config = femr.models.transformer.FEMRTransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    is_hierarchical=tokenizer.is_hierarchical, \n",
    "    n_layers=2,\n",
    "    hidden_size=64, \n",
    "    intermediate_size=64*2,\n",
    "    n_heads=8,\n",
    ")\n",
    "\n",
    "config = femr.models.transformer.FEMRModelConfig.from_transformer_task_configs(transformer_config, motor_task.get_task_config())\n",
    "\n",
    "model = femr.models.transformer.FEMRModel(config)\n",
    "\n",
    "collator = processor.collate\n",
    "\n",
    "trainer_config = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "\n",
    "    output_dir='tmp_trainer',\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "\n",
    "    eval_steps=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    logging_steps=20,\n",
    "    logging_strategy='steps',\n",
    "\n",
    "    prediction_loss_only=True,\n",
    "    \n",
    "    report_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290a4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    # data_collator=processor.collate,\n",
    "    train_dataset=train_batches['train'],\n",
    "    eval_dataset=train_batches['test'],\n",
    "    args=trainer_config,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3301a42",
   "metadata": {},
   "source": [
    "model.save_pretrained(os.path.join(TARGET_DIR, 'motor_model'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "552dc83c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FEMR_ENV",
   "language": "python",
   "name": "femr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
