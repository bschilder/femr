{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69606f3f-eff1-42ba-9b76-ace8020b452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "132702e4-d938-46c6-b5ab-6246664b04cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import piton\n",
    "import piton.datasets\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForMaskedLM\n",
    "from typing import Tuple, List\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad95eca8-4d1c-47c3-9096-6429fbe82bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_file(path_to_file: str):\n",
    "    \"\"\"Load object from Pickle file.\"\"\"\n",
    "    with open(path_to_file, \"rb\") as fd:\n",
    "        result = pickle.load(fd)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c02c9c-7aed-40c6-b005-fffede392c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /local-scratch/nigam/projects/clmbr_text_assets/models/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "path_to_model = \"/local-scratch/nigam/projects/clmbr_text_assets/models/Bio_ClinicalBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "model = AutoModel.from_pretrained(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2228d79-0de5-4c64-a12f-f3b807f535ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "padding = True\n",
    "truncation = True\n",
    "\n",
    "# Run notes through an already-trained tokenizer\n",
    "notes_tokenized = tokenizer(\n",
    "    [note.value for note in notes],\n",
    "    padding=padding,\n",
    "    truncation=truncation,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085ce0d-dc30-4270-aea1-83792fbece59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "max_length = 512\n",
    "padding = True\n",
    "truncation = True\n",
    "\n",
    "# Run notes through an already-trained tokenizer\n",
    "notes_tokenized = tokenizer(\n",
    "    [note.value for note in notes],\n",
    "    padding=padding,\n",
    "    truncation=truncation,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(args.get(\"path_to_model\"))\n",
    "\n",
    "outputs = model(**notes_tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3015bb4b-16ab-4b97-bffc-c5dd67509bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_patients = load_from_file(\"/local-scratch/nigam/projects/rthapa84/data/HighHbA1c_labeled_patients_v3.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37a02e53-0016-466c-b4b2-d7fbca82b267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "418465"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b7aa5ed-a897-440b-8aee-43ab106d1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 10\n",
    "\n",
    "database_path = \"/local-scratch/nigam/projects/ethanid/som-rit-phi-starr-prod.starr_omop_cdm5_deid_2022_09_05_extract2\"\n",
    "\n",
    "database = piton.datasets.PatientDatabase(database_path)\n",
    "pids = sorted(labeled_patients.get_all_patient_ids())\n",
    "\n",
    "pids = pids[:40]\n",
    "\n",
    "pids_parts = np.array_split(pids, num_threads)\n",
    "\n",
    "tasks = [(database_path, pid_part, labeled_patients) for pid_part in pids_parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbea754a-edef-4aa3-80b1-3df85a4e65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path, pids, labeled_patients = tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfaf97ef-a38e-4ddc-be7f-02c0aacbc63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = piton.datasets.PatientDatabase(database_path)\n",
    "ontology = database.get_ontology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dcb6d66-e73d-490d-8185-ab942f0969c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHAR = 100\n",
    "# for event in database[0].events:\n",
    "#     if type(event.value) == memoryview:\n",
    "#         text = bytes(event.value).decode(\"utf-8\")\n",
    "        \n",
    "#         if len(text) < NUM_CHAR:\n",
    "#             continue\n",
    "#         print(text)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f454d31-6210-4d79-9e84-b52876c483c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mu Name is'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([\"Mu\", \"Name\", \"is\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa378212-4cb3-46ca-9e2a-25653136f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [i for i in range(1239)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dad1f-3f01-499e-8a5c-5914c2241653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56bcbee-b57a-4721-ac9e-72b7ae7ba11d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "927a4d8f-2341-4221-bb64-20b5eb159a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_patient_text_data(patient, labels):\n",
    "    text_for_each_label = []\n",
    "\n",
    "    label_idx = 0\n",
    "    current_text = []\n",
    "    for event in patient.events:\n",
    "        while event.start > labels[label_idx].time:\n",
    "            label_idx += 1\n",
    "            text_for_each_label.append(\" \".join(current_text))\n",
    "\n",
    "            if label_idx >= len(labels):\n",
    "                return text_for_each_label\n",
    "\n",
    "        if type(event.value) is not memoryview:\n",
    "            continue\n",
    "\n",
    "        text_data = bytes(event.value).decode(\"utf-8\")\n",
    "\n",
    "        if len(text_data) < MAX_CHAR:\n",
    "            continue\n",
    "\n",
    "        current_text.append(text_data)\n",
    "\n",
    "    if label_idx < len(labels):\n",
    "        for label in labels[label_idx:]:\n",
    "            text_for_each_label.append(\" \".join(current_text))\n",
    "\n",
    "\n",
    "    return text_for_each_label\n",
    "    \n",
    "\n",
    "\n",
    "def _run_text_featurizer(database_path, pids, labeled_patients, path_to_model):\n",
    "    \n",
    "    # database_path, pids, labeled_patients, path_to_model = args\n",
    "    database = piton.datasets.PatientDatabase(database_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "    model = AutoModel.from_pretrained(path_to_model)\n",
    "    max_length = 512\n",
    "    padding = True\n",
    "    truncation = True\n",
    "    CHUNK_SIZE = 10\n",
    "    \n",
    "    data = []\n",
    "    patient_ids = []\n",
    "    result_labels = []\n",
    "    labeling_time = []\n",
    "    \n",
    "    for patient_id in pids:\n",
    "        patient = database[patient_id]\n",
    "        labels = labeled_patients.pat_idx_to_label(patient_id)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            continue\n",
    "        \n",
    "        patient_text_data = _get_patient_text_data(patient, labels)\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            data.append(patient_text_data[i])\n",
    "            result_labels.append(label.value)\n",
    "            patient_ids.append(patient.patient_id)\n",
    "            labeling_time.append(label.time)\n",
    "    \n",
    "    embeddings = []\n",
    "    for chunk in range(0, len(data), CHUNK_SIZE):\n",
    "        notes_tokenized = tokenizer(\n",
    "                                data[chunk:chunk+CHUNK_SIZE],\n",
    "                                padding=padding,\n",
    "                                truncation=truncation,\n",
    "                                max_length=max_length,\n",
    "                                return_tensors=\"pt\",\n",
    "                            )\n",
    "        outputs = model(**notes_tokenized)\n",
    "        batch_embedding_tensor = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "        batch_embedding_numpy = batch_embedding_tensor.cpu().detach().numpy()\n",
    "        embeddings.append(batch_embedding_numpy)\n",
    "    \n",
    "    embeddings = np.concatenate(embeddings)\n",
    "            \n",
    "    return embeddings, result_labels, patient_ids, labeling_time\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed9ce2-2f4e-4ba6-9eff-0a47a99da121",
   "metadata": {},
   "outputs": [],
   "source": [
    "pids = sorted(labeled_patients.get_all_patient_ids())[:100]\n",
    "num_threads = 5\n",
    "\n",
    "pids_parts = np.array_split(pids, num_threads)\n",
    "\n",
    "tasks = [(database_path, pid_part, labeled_patients, path_to_model) for pid_part in pids_parts]\n",
    "\n",
    "ctx = multiprocessing.get_context('forkserver')\n",
    "with ctx.Pool(num_threads) as pool:\n",
    "    text_featurizers_tuple_list = list(pool.imap(_run_text_featurizer, tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e733a-bad6-4274-838a-a9ff4db0f3ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed998b57-3a77-4b0e-b240-46aeba19f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /local-scratch/nigam/projects/clmbr_text_assets/models/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embeddings, _, _, _ = _run_text_featurizer(database_path, pids, labeled_patients, path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49c8402d-e12b-43ad-a3b9-453edfb1aabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac752bd-3fd2-43e2-bcff-3fbbfbce058e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fffaf27-b8a6-46ef-bde5-cfb91536d1b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0a1cfd6-0cc2-41a8-bc55-22b7d824139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /local-scratch/nigam/projects/clmbr_text_assets/models/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "max_length = 512\n",
    "padding = True\n",
    "truncation = True\n",
    "\n",
    "# Run notes through an already-trained tokenizer\n",
    "notes_tokenized = tokenizer(\n",
    "    data,\n",
    "    padding=padding,\n",
    "    truncation=truncation,\n",
    "    max_length=max_length,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(path_to_model)\n",
    "\n",
    "outputs = model(**notes_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "071286ca-cce8-4d1d-bbc3-58131b3992f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_with_cls(embeddings):\n",
    "    return embeddings[:, 0, :].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b9b5fd9-b9a3-4e25-a4f2-badb5cec3c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_with_cls(outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9caf2d30-811f-417b-bac6-73292443a119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27650014, -0.013582  , -0.66790944, ...,  0.05187273,\n",
       "         0.1168906 ,  0.02797246],\n",
       "       [-0.11857603,  0.25548074, -0.6114352 , ..., -0.08299585,\n",
       "         0.17861778, -0.36386025],\n",
       "       [ 0.36844736,  0.3425017 , -0.5480165 , ..., -0.3408979 ,\n",
       "         0.60281944, -0.22029027],\n",
       "       [ 0.22482194, -0.24363495, -0.03791487, ...,  0.08538163,\n",
       "         0.39754403,  0.08916175]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:, 0, :].squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "983319e8-27c9-4682-ad9c-4a128c911b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1218, 2027,  ..., 1336, 4506,  102],\n",
       "        [ 101, 4113, 3516,  ..., 5682,  129,  102],\n",
       "        [ 101, 3372, 1106,  ...,    0,    0,    0],\n",
       "        [ 101,  102,    0,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1736e5c5-328e-406c-ba21-aad70d9e1811",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = 10\n",
    "patient = database[patient_id]\n",
    "labels = labeled_patients.pat_idx_to_label(patient_id)\n",
    "\n",
    "text_1 = _get_patient_text_data(patient, labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "560a6624-1508-47f2-90a0-048fe17f3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Label(time=datetime.datetime(2019, 10, 10, 7, 49), value=False)]\n",
      "\n",
      "[Label(time=datetime.datetime(2021, 7, 3, 10, 23), value=False)]\n",
      "\n",
      "[Label(time=datetime.datetime(2017, 10, 13, 11, 11), value=False)]\n",
      "\n",
      "[Label(time=datetime.datetime(2017, 4, 18, 11, 35), value=True)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for patient_id in pids:\n",
    "    patient = database[patient_id]\n",
    "    labels = labeled_patients.pat_idx_to_label(patient_id)\n",
    "    \n",
    "    if len(labels) == 0:\n",
    "        continue\n",
    "        \n",
    "    data = []\n",
    "    result_labels = []\n",
    "    patient_ids = []\n",
    "    labeling_time = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    label_idx = 0\n",
    "    current_text = []\n",
    "    for event in patient.events:\n",
    "        while event.start > labels[label_idx].time:\n",
    "            label_idx += 1\n",
    "            text_for_each_label.append(\" \".join(current_text))\n",
    "\n",
    "            if label_idx >= len(labels):\n",
    "                return text_for_each_label\n",
    "\n",
    "        if type(event.value) is not memoryview:\n",
    "            continue\n",
    "            \n",
    "        text_data = bytes(event.value).decode(\"utf-8\")\n",
    "        \n",
    "        if len(text_data) < MAX_CHAR:\n",
    "            continue\n",
    "        \n",
    "        current_text.append(text_data)\n",
    "\n",
    "    if label_idx < len(labels):\n",
    "        for label in labels[label_idx:]:\n",
    "            text_for_each_label.append(\" \".join(current_text))\n",
    "        \n",
    "    \n",
    "    return text_for_each_label\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee197f16-9a28-40a2-bbbd-e57be5a803ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529001c-ef01-45a2-9612-fa297bcaade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_featurizer(args: Tuple[str, List[int], labeled_patients, List[Featurizer]]) -> Tuple[Any, Any, Any, Any]:\n",
    "\n",
    "    # print(\"launched\")\n",
    "    data = []\n",
    "    indices: List[int] = []\n",
    "    indptr = []\n",
    "\n",
    "    result_labels = []\n",
    "    patient_ids = []\n",
    "    labeling_time = []\n",
    "\n",
    "    database_path, pids, labeled_patients, featurizers = args\n",
    "\n",
    "    database = PatientDatabase(database_path)\n",
    "    ontology = database.get_ontology()\n",
    "\n",
    "    for patient_id in pids:\n",
    "        # print(\"launched\", patient_id)\n",
    "        patient = database[patient_id]\n",
    "        labels = labeled_patients.pat_idx_to_label(patient_id)\n",
    "\n",
    "        if len(labels) == 0:\n",
    "            continue\n",
    "\n",
    "        columns_by_featurizer = []\n",
    "\n",
    "        for featurizer in featurizers:\n",
    "            columns = featurizer.featurize(patient, labels, ontology)\n",
    "            assert len(columns) == len(labels), (\n",
    "                f\"The featurizer {featurizer} didn't provide enough rows for \"\n",
    "                f\"{labeling_function} on patient {patient.patient_id} ({len(columns)} != {len(labels)})\"\n",
    "            )\n",
    "            columns_by_featurizer.append(columns)\n",
    "\n",
    "        for i, label in enumerate(labels):\n",
    "            indptr.append(len(indices))\n",
    "            result_labels.append(label.value)\n",
    "            patient_ids.append(patient.patient_id)\n",
    "            labeling_time.append(label.time)\n",
    "\n",
    "            column_offset = 0\n",
    "            for j, feature_columns in enumerate(columns_by_featurizer):\n",
    "                for column, value in feature_columns[i]:\n",
    "                    assert (\n",
    "                        0 <= column < featurizers[j].num_columns()\n",
    "                    ), (\n",
    "                        f\"The featurizer {featurizers[j]} provided an out of bounds column for \"\n",
    "                        f\"{labeling_function} on patient {patient.patient_id} ({column} should be between 0 and \"\n",
    "                        f\"{featurizers[j].num_columns()})\"\n",
    "                    )\n",
    "                    indices.append(column_offset + column)\n",
    "                    data.append(value)\n",
    "\n",
    "                column_offset += featurizers[j].num_columns()\n",
    "    indptr.append(len(indices))\n",
    "\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    indices = np.array(indices, dtype=np.int32)\n",
    "    indptr = np.array(indptr, dtype=np.int32)\n",
    "    result_labels = np.array(result_labels)\n",
    "    patient_ids = np.array(patient_ids, dtype=np.int32)\n",
    "    labeling_time = np.array(patient_ids, dtype=np.datetime64)\n",
    "\n",
    "    total_columns = sum(\n",
    "        featurizer.num_columns() for featurizer in featurizers\n",
    "    )\n",
    "\n",
    "    data_matrix = scipy.sparse.csr_matrix(\n",
    "        (data, indices, indptr), shape=(len(result_labels), total_columns)\n",
    "    )\n",
    "\n",
    "    # print(\"Done\", data_matrix.shape)\n",
    "\n",
    "    # data_matrix.check_format() # remove when we think its works\n",
    "\n",
    "    # print(data_matrix.shape, result_labels.shape, patient_ids.shape, labeling_time.shape)\n",
    "\n",
    "    return data_matrix, result_labels, patient_ids, labeling_time\n",
    "\n",
    "\n",
    "class FeaturizerList:\n",
    "    \"\"\"\n",
    "    Featurizer list consists of a list of featurizers that will be used (in sequence) to featurize data.\n",
    "    It enables preprocessing of featurizers, featurization, and column name extraction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, featurizers: List[Featurizer]):\n",
    "        \"\"\"Create a :class:`FeaturizerList` from a sequence of featurizers.\n",
    "\n",
    "        Args:\n",
    "            featurizers (List[Featurizer]): The featurizers to use for featurizeing patients.\n",
    "        \"\"\"\n",
    "        self.featurizers = featurizers\n",
    "\n",
    "    def preprocess_featurizers(\n",
    "        self,\n",
    "        # patients: Sequence[Patient],\n",
    "        labeled_patients: LabeledPatients,\n",
    "        database_path: str,\n",
    "        num_threads: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"preprocess a list of featurizers on the provided patients using the given labeler.\n",
    "\n",
    "        Args:\n",
    "            patients (List[Patient]): Sequence of patients.\n",
    "            labeling_function (:class:`labelers.core.LabelingFunction`): The labeler to preprocess with.\n",
    "        \"\"\"\n",
    "\n",
    "        any_needs_preprocessing = any(\n",
    "            featurizer.needs_preprocessing() for featurizer in self.featurizers\n",
    "        )\n",
    "\n",
    "        if not any_needs_preprocessing:\n",
    "            return\n",
    "\n",
    "        pids = sorted(labeled_patients.get_all_patient_ids())\n",
    "\n",
    "        pids_parts = np.array_split(pids, num_threads)\n",
    "\n",
    "        tasks = [(database_path, pid_part, labeled_patients, self.featurizers) for pid_part in pids_parts]\n",
    "\n",
    "        ctx = multiprocessing.get_context('forkserver')\n",
    "        with ctx.Pool(num_threads) as pool:\n",
    "            trained_featurizers_tuple_list = list(pool.imap(_run_preprocess_featurizers, tasks))\n",
    "\n",
    "        age_featurizers = []\n",
    "        count_featurizers = []\n",
    "\n",
    "        for trained_featurizers_tuple in trained_featurizers_tuple_list:\n",
    "            age_featurizers.append(trained_featurizers_tuple[0])\n",
    "            count_featurizers.append(trained_featurizers_tuple[1])\n",
    "\n",
    "        # Aggregating age featurizers\n",
    "        for age_featurizer in age_featurizers:\n",
    "            if age_featurizer.to_dict()[\"age_statistics\"][\"current_mean\"] != 0:\n",
    "                self.featurizers[0].from_dict(age_featurizer.to_dict())\n",
    "                break\n",
    "\n",
    "        # Aggregating count featurizers\n",
    "        patient_codes_dict_list = [count_featurizer.to_dict()[\"patient_codes\"][\"values\"] for count_featurizer in count_featurizers]\n",
    "        patient_codes = list(itertools.chain.from_iterable(patient_codes_dict_list))\n",
    "        self.featurizers[1].from_dict({\"patient_codes\": {\"values\": patient_codes}})\n",
    "\n",
    "        for featurizer in self.featurizers:\n",
    "            featurizer.finalize_preprocessing()\n",
    "\n",
    "    def featurize(\n",
    "        self,\n",
    "        labeled_patients: LabeledPatients,\n",
    "        database_path: str,\n",
    "        num_threads: int = 1,\n",
    "    ) -> Tuple[Any, Any, Any, Any]:\n",
    "        \"\"\"\n",
    "        Apply a list of featurizers to obtain a feature matrix and label vector for the given patients.\n",
    "        Args:\n",
    "            patients (List[Patient]): Sequence of patients\n",
    "            labeling_function (:class:`labelers.core.LabelingFunction`): The labeler to preprocess with.\n",
    "        Returns:\n",
    "            This returns a tuple (data_matrix, labels, patient_ids, labeling_time).\n",
    "            data_matrix is a sparse matrix of all the features of all the featurizers.\n",
    "            labels is a list of boolean values representing the labels for each row in the matrix.\n",
    "            patient_ids is a list of the patient ids for each row.\n",
    "            labeling_time is a list of labeling/prediction time for each row.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO check what is happening here\n",
    "        # print(len(labeled_patients.get_all_patient_ids()))\n",
    "        pids = sorted(labeled_patients.get_all_patient_ids())\n",
    "        # print(pids)\n",
    "\n",
    "        # pids = [i for i in range(len(patients))]\n",
    "        pids_parts = np.array_split(pids, num_threads)\n",
    "\n",
    "        tasks = [(database_path, pid_part, labeled_patients, self.featurizers) for pid_part in pids_parts]\n",
    "\n",
    "        # multiprocessing.set_start_method('spawn', force=True)\n",
    "        # print(\"This is before lunch\", len(tasks))\n",
    "        ctx = multiprocessing.get_context('forkserver')\n",
    "        with ctx.Pool(num_threads) as pool:\n",
    "            results = list(pool.imap(_run_featurizer, tasks))\n",
    "        # print(\"Finished multiprocessing\")\n",
    "\n",
    "        data_matrix_list = []\n",
    "        result_labels_list = []\n",
    "        patient_ids_list = []\n",
    "        labeling_time_list = []\n",
    "        for result in results:\n",
    "            # if result[0].shape[0] != 0:\n",
    "            data_matrix_list.append(result[0])\n",
    "            # if result[1].shape[1] != 0:\n",
    "            result_labels_list.append(result[1])\n",
    "            patient_ids_list.append(result[2])\n",
    "            labeling_time_list.append(result[3])\n",
    "        \n",
    "        data_matrix = scipy.sparse.vstack(data_matrix_list)\n",
    "        result_labels = np.concatenate(result_labels_list, axis=None)\n",
    "        patient_ids = np.concatenate(patient_ids_list, axis=None)\n",
    "        labeling_time = np.concatenate(labeling_time_list, axis=None)\n",
    "\n",
    "        return (\n",
    "            data_matrix,\n",
    "            result_labels,\n",
    "            patient_ids,\n",
    "            labeling_time,\n",
    "        )\n",
    "\n",
    "    def get_column_name(self, column_index: int) -> str:\n",
    "        offset = 0\n",
    "\n",
    "        for featurizer in self.featurizers:\n",
    "            if offset <= column_index < (offset + featurizer.num_columns()):\n",
    "                return f\"Featurizer {featurizer}, {featurizer.get_column_name(column_index - offset)}\"\n",
    "\n",
    "            offset += featurizer.num_columns()\n",
    "\n",
    "        assert False, \"This should never happen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2c94af-15e0-421d-af13-6782eac1b1b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlabeled_patients\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/local-scratch/nigam/projects/rthapa84/repos/piton/src/piton/labelers/core.py:281\u001b[0m, in \u001b[0;36mLabeledPatients.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124;03m\"\"\"Necessary for implementing MutableMapping.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatients_to_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "class CountFeaturizer(Featurizer):\n",
    "    \"\"\"\n",
    "    Produces one column per each diagnosis code, procedure code, and prescription code.\n",
    "    The value in each column is the count of how many times that code appears in the patient record\n",
    "    before the corresponding label.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        # ontology: extension_datasets.Ontology,\n",
    "        rollup: bool = False,\n",
    "        exclusion_codes: List[int] = [],\n",
    "        time_bins: Optional[\n",
    "            List[Optional[int]]\n",
    "        ] = None,  # [90, 180] refers to [0-90, 90-180]; [90, 180, math.inf] refers to [0-90, 90-180, 180-inf]\n",
    "    ):\n",
    "        self.patient_codes: Dictionary = Dictionary()\n",
    "        self.exclusion_codes = set(exclusion_codes)\n",
    "        self.time_bins = time_bins\n",
    "        # self.ontology = ontology\n",
    "        self.rollup = rollup\n",
    "\n",
    "    def get_codes(self, code: int, ontology: extension_datasets.Ontology) -> Iterator[int]:\n",
    "        if code not in self.exclusion_codes:\n",
    "            if self.rollup:\n",
    "                for subcode in ontology.get_all_parents(code):\n",
    "                    yield subcode\n",
    "            else:\n",
    "                yield code\n",
    "\n",
    "    def preprocess(self, patient: Patient, labels: List[Label]):\n",
    "        \"\"\"Adds every event code in this patient's timeline to `patient_codes`\"\"\"\n",
    "        for event in patient.events:\n",
    "            if event.value is None:\n",
    "                self.patient_codes.add(event.code)\n",
    "\n",
    "    def num_columns(self) -> int:\n",
    "        if self.time_bins is None:\n",
    "            return len(self.patient_codes)\n",
    "        else:\n",
    "            return len(self.time_bins) * len(self.patient_codes)\n",
    "\n",
    "    def featurize(\n",
    "        self, patient: Patient, labels: List[Label], ontology: extension_datasets.Ontology,\n",
    "    ) -> List[List[ColumnValue]]:\n",
    "        all_columns: List[List[ColumnValue]] = []\n",
    "\n",
    "        if self.time_bins is None:\n",
    "            current_codes: Dict[int, int] = defaultdict(int)\n",
    "\n",
    "            label_idx = 0\n",
    "            for event in patient.events:\n",
    "                while event.start > labels[label_idx].time:\n",
    "                    label_idx += 1\n",
    "                    all_columns.append(\n",
    "                        [\n",
    "                            ColumnValue(column, count)\n",
    "                            for column, count in current_codes.items()\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    if label_idx >= len(labels):\n",
    "                        return all_columns\n",
    "\n",
    "                if event.value is not None:\n",
    "                    continue\n",
    "\n",
    "                for code in self.get_codes(event.code, ontology):\n",
    "                    if code in self.patient_codes:\n",
    "                        current_codes[self.patient_codes.transform(code)] += 1\n",
    "\n",
    "            if label_idx < len(labels):\n",
    "                for label in labels[label_idx:]:\n",
    "                    all_columns.append(\n",
    "                        [\n",
    "                            ColumnValue(column, count)\n",
    "                            for column, count in current_codes.items()\n",
    "                        ]\n",
    "                    )\n",
    "                \n",
    "\n",
    "                # if label_idx == len(labels) - 1:\n",
    "                #     all_columns.append(\n",
    "                #         [\n",
    "                #             ColumnValue(column, count)\n",
    "                #             for column, count in current_codes.items()\n",
    "                #         ]\n",
    "                #     )\n",
    "                #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38caae-ea60-4007-9360-bbe2f1e9a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFeaturizer(Featurizer):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
