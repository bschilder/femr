{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c59464-ef8f-4fb9-a0a6-ebb68475e2a9",
   "metadata": {},
   "source": [
    "# Using CLMBR to generate features and training models on those features\n",
    "\n",
    "We can use a trained CLMBR model to generate features and then use those features in a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe93d59d-f135-46f6-b0a7-2d75d9b18e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_7'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d741a7-46a2-4760-a369-3efb01afd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc07519fbf64393b42e326b15a062a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at input/motor_model were not used when initializing FEMRModel: ['task_model.final_layer.bias', 'task_model.final_layer.weight', 'task_model.task_embedding.weight', 'task_model.task_embedding_bias.weight', 'transformer.in_norm.scale', 'transformer.layers.0.norm.scale', 'transformer.layers.1.norm.scale', 'transformer.out_norm.scale']\n",
      "- This IS expected if you are initializing FEMRModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FEMRModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FEMRModel were not initialized from the model checkpoint at input/motor_model and are newly initialized: ['transformer.in_norm.weight', 'transformer.layers.0.norm.weight', 'transformer.layers.1.norm.weight', 'transformer.out_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'femr.models.transformer.FEMRModel'> model type\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c320c3810c043e88305a05bdc95faa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1135adcd29147ba94b1b1b61a46d78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_patients': tensor([64]), 'num_indices': tensor([64]), 'patient_ids': tensor([[ 88,  88, 122, 122,  70,  70, 174, 174, 194, 194,  87,  87,  86,  86,\n",
      "          92,  92,  73,  73,  27,  27, 112, 112, 117, 117, 135, 135,  54,  54,\n",
      "          62,  62, 141, 141, 177, 177, 107, 107,  50,  50,  99,  99, 189, 189,\n",
      "          36,  36, 173, 173, 191, 191,  35,  35,  89,  89, 181, 181,  77,  77,\n",
      "         197, 197,  93,  93,  28,  28,  67,  67,  48,  48, 105, 105,  29,  29,\n",
      "          57,  57,  69,  69, 120, 120, 198, 198,  72,  72,  12,  12,  43,  43,\n",
      "         106, 106,  83,  83,  49,  49, 140, 140, 190, 190, 167, 167,  95,  95,\n",
      "          45,  45,  74,  74,  40,  40,  26,  26,   3,   3,   5,   5,  58,  58,\n",
      "          20,  20, 130, 130,  34,  34, 172, 172,  19,  19, 123, 123, 121, 121,\n",
      "         182, 182]]), 'offsets': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'transformer': {'ages': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]]), 'hierarchical_tokens': tensor([[1, 3, 1, 2, 0, 3, 0, 3, 0, 3, 1, 2, 0, 3, 1, 2, 0, 3, 1, 3, 0, 3, 1, 3,\n",
      "         0, 2, 1, 2, 0, 3, 1, 2, 1, 3, 1, 3, 0, 3, 0, 3, 1, 3, 0, 2, 1, 2, 1, 2,\n",
      "         1, 2, 0, 3, 1, 3, 1, 3, 0, 3, 1, 3, 1, 3, 0, 3, 0, 2, 0, 2, 0, 3, 0, 3,\n",
      "         1, 3, 1, 3, 0, 3, 0, 3, 1, 3, 0, 3, 0, 2, 1, 2, 0, 2, 0, 3, 0, 3, 0, 2,\n",
      "         0, 2, 0, 2, 0, 3, 1, 2, 0, 3, 0, 2, 0, 3, 0, 2, 0, 2, 0, 2, 1, 3, 1, 2,\n",
      "         1, 3, 1, 2, 0, 3, 1, 2]]), 'hierarchical_weights': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]]), 'label_indices': tensor([[  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "          29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "          57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "          85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "         113, 115, 117, 119, 121, 123, 125, 127]]), 'normalized_ages': tensor([[-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204]]), 'patient_lengths': array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]), 'timestamps': tensor([[684720000, 684720000, 683078400, 683078400, 717292800, 717292800,\n",
      "         676944000, 676944000, 667440000, 667440000, 688694400, 688694400,\n",
      "         658972800, 658972800, 706320000, 706320000, 709430400, 709430400,\n",
      "         688348800, 688348800, 676771200, 676771200, 693532800, 693532800,\n",
      "         695692800, 695692800, 650246400, 650246400, 692928000, 692928000,\n",
      "         669945600, 669945600, 712368000, 712368000, 707097600, 707097600,\n",
      "         704678400, 704678400, 704246400, 704246400, 707011200, 707011200,\n",
      "         671500800, 671500800, 717552000, 717552000, 669686400, 669686400,\n",
      "         652492800, 652492800, 695433600, 695433600, 679190400, 679190400,\n",
      "         712368000, 712368000, 660441600, 660441600, 709516800, 709516800,\n",
      "         674438400, 674438400, 704937600, 704937600, 684288000, 684288000,\n",
      "         646617600, 646617600, 705715200, 705715200, 681091200, 681091200,\n",
      "         641606400, 641606400, 671932800, 671932800, 716688000, 716688000,\n",
      "         680659200, 680659200, 675820800, 675820800, 699840000, 699840000,\n",
      "         706752000, 706752000, 684028800, 684028800, 649900800, 649900800,\n",
      "         652492800, 652492800, 670118400, 670118400, 714700800, 714700800,\n",
      "         694915200, 694915200, 667180800, 667180800, 666921600, 666921600,\n",
      "         646444800, 646444800, 646012800, 646012800, 641001600, 641001600,\n",
      "         706060800, 706060800, 644803200, 644803200, 669427200, 669427200,\n",
      "         650419200, 650419200, 686707200, 686707200, 703555200, 703555200,\n",
      "         716515200, 716515200, 662256000, 662256000, 652320000, 652320000,\n",
      "         709776000, 709776000]]), 'token_indices': tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128]]), 'valid_tokens': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]])}, 'task': {'patient_ids': tensor([[ 88, 122,  70, 174, 194,  87,  86,  92,  73,  27, 112, 117, 135,  54,\n",
      "          62, 141, 177, 107,  50,  99, 189,  36, 173, 191,  35,  89, 181,  77,\n",
      "         197,  93,  28,  67,  48, 105,  29,  57,  69, 120, 198,  72,  12,  43,\n",
      "         106,  83,  49, 140, 190, 167,  95,  45,  74,  40,  26,   3,   5,  58,\n",
      "          20, 130,  34, 172,  19, 123, 121, 182]]), 'prediction_timestamps': tensor([[684720000, 683078400, 717292800, 676944000, 667440000, 688694400,\n",
      "         658972800, 706320000, 709430400, 688348800, 676771200, 693532800,\n",
      "         695692800, 650246400, 692928000, 669945600, 712368000, 707097600,\n",
      "         704678400, 704246400, 707011200, 671500800, 717552000, 669686400,\n",
      "         652492800, 695433600, 679190400, 712368000, 660441600, 709516800,\n",
      "         674438400, 704937600, 684288000, 646617600, 705715200, 681091200,\n",
      "         641606400, 671932800, 716688000, 680659200, 675820800, 699840000,\n",
      "         706752000, 684028800, 649900800, 652492800, 670118400, 714700800,\n",
      "         694915200, 667180800, 666921600, 646444800, 646012800, 641001600,\n",
      "         706060800, 644803200, 669427200, 650419200, 686707200, 703555200,\n",
      "         716515200, 662256000, 652320000, 709776000]])}, 'needs_exact': tensor([True])} batch, compute_features\n",
      "label_indices tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "        113, 115, 117, 119, 121, 123, 125, 127])\n",
      "normalized_ages tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204])\n",
      "before in norm torch.Size([128, 64])\n",
      "after in norm torch.Size([128, 64])\n",
      "pos_embed torch.Size([128, 1, 8]) torch.Size([128, 1, 8])\n",
      "x in each layer torch.Size([128, 64])\n",
      "x in each layer torch.Size([128, 64])\n",
      "final torch.Size([128, 64])\n",
      "FEMRModel label_indices tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "        113, 115, 117, 119, 121, 123, 125, 127])\n",
      "FEMRModel features torch.Size([128, 64])\n",
      "all_patient_ids torch.Size([64]) feature_times (64,) representations torch.Size([64, 64])\n",
      "{'num_patients': tensor([64]), 'num_indices': tensor([64]), 'patient_ids': tensor([[128, 128, 109, 109, 199, 199, 114, 114, 147, 147, 129, 129,  85,  85,\n",
      "          10,  10, 186, 186,  23,  23, 139, 139, 126, 126,   0,   0, 185, 185,\n",
      "          59,  59,  13,  13, 192, 192,   9,   9,  39,  39, 168, 168, 195, 195,\n",
      "         165, 165, 108, 108, 153, 153,  55,  55,  82,  82,  42,  42, 158, 158,\n",
      "           8,   8, 124, 124,  65,  65,  44,  44,  68,  68, 184, 184, 142, 142,\n",
      "          41,  41, 134, 134, 159, 159, 155, 155, 102, 102,   6,   6,  71,  71,\n",
      "          64,  64, 113, 113, 187, 187,  75,  75,  33,  33,  17,  17,  79,  79,\n",
      "         188, 188, 149, 149,  97,  97,  14,  14, 143, 143,  90,  90,  22,  22,\n",
      "         180, 180,  30,  30, 179, 179, 166, 166,  37,  37,  18,  18, 116, 116,\n",
      "         170, 170]]), 'offsets': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'transformer': {'ages': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]]), 'hierarchical_tokens': tensor([[1, 2, 0, 2, 0, 2, 0, 3, 0, 3, 1, 2, 1, 3, 1, 2, 1, 2, 1, 3, 0, 3, 0, 3,\n",
      "         1, 2, 1, 3, 1, 3, 1, 2, 0, 3, 1, 2, 1, 3, 1, 2, 1, 3, 0, 2, 0, 3, 0, 3,\n",
      "         0, 3, 1, 2, 0, 2, 1, 3, 1, 2, 1, 2, 0, 2, 1, 2, 0, 3, 1, 3, 1, 2, 0, 3,\n",
      "         0, 3, 0, 2, 1, 3, 0, 2, 0, 3, 1, 2, 1, 2, 0, 2, 1, 2, 1, 2, 0, 3, 0, 2,\n",
      "         1, 2, 0, 3, 1, 2, 1, 3, 0, 3, 0, 3, 1, 3, 0, 3, 0, 2, 1, 3, 1, 2, 1, 2,\n",
      "         0, 2, 1, 3, 0, 3, 0, 2]]), 'hierarchical_weights': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]]), 'label_indices': tensor([[  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "          29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "          57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "          85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "         113, 115, 117, 119, 121, 123, 125, 127]]), 'normalized_ages': tensor([[-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204]]), 'patient_lengths': array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]), 'timestamps': tensor([[690336000, 690336000, 677808000, 677808000, 703209600, 703209600,\n",
      "         698284800, 698284800, 675216000, 675216000, 707961600, 707961600,\n",
      "         643420800, 643420800, 689731200, 689731200, 694051200, 694051200,\n",
      "         704246400, 704246400, 715046400, 715046400, 678499200, 678499200,\n",
      "         696297600, 696297600, 646531200, 646531200, 659318400, 659318400,\n",
      "         653097600, 653097600, 644112000, 644112000, 686534400, 686534400,\n",
      "         712540800, 712540800, 652924800, 652924800, 697852800, 697852800,\n",
      "         640310400, 640310400, 687916800, 687916800, 696643200, 696643200,\n",
      "         689817600, 689817600, 673228800, 673228800, 690336000, 690336000,\n",
      "         667094400, 667094400, 685843200, 685843200, 658022400, 658022400,\n",
      "         675993600, 675993600, 684374400, 684374400, 647568000, 647568000,\n",
      "         706492800, 706492800, 661219200, 661219200, 713145600, 713145600,\n",
      "         640569600, 640569600, 707702400, 707702400, 678672000, 678672000,\n",
      "         671500800, 671500800, 650332800, 650332800, 648604800, 648604800,\n",
      "         709603200, 709603200, 645062400, 645062400, 711072000, 711072000,\n",
      "         644716800, 644716800, 710553600, 710553600, 690595200, 690595200,\n",
      "         647049600, 647049600, 651542400, 651542400, 654393600, 654393600,\n",
      "         652060800, 652060800, 688089600, 688089600, 648432000, 648432000,\n",
      "         712800000, 712800000, 668908800, 668908800, 701913600, 701913600,\n",
      "         715737600, 715737600, 642297600, 642297600, 640915200, 640915200,\n",
      "         706320000, 706320000, 692150400, 692150400, 710985600, 710985600,\n",
      "         654307200, 654307200]]), 'token_indices': tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128]]), 'valid_tokens': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]])}, 'task': {'patient_ids': tensor([[128, 109, 199, 114, 147, 129,  85,  10, 186,  23, 139, 126,   0, 185,\n",
      "          59,  13, 192,   9,  39, 168, 195, 165, 108, 153,  55,  82,  42, 158,\n",
      "           8, 124,  65,  44,  68, 184, 142,  41, 134, 159, 155, 102,   6,  71,\n",
      "          64, 113, 187,  75,  33,  17,  79, 188, 149,  97,  14, 143,  90,  22,\n",
      "         180,  30, 179, 166,  37,  18, 116, 170]]), 'prediction_timestamps': tensor([[690336000, 677808000, 703209600, 698284800, 675216000, 707961600,\n",
      "         643420800, 689731200, 694051200, 704246400, 715046400, 678499200,\n",
      "         696297600, 646531200, 659318400, 653097600, 644112000, 686534400,\n",
      "         712540800, 652924800, 697852800, 640310400, 687916800, 696643200,\n",
      "         689817600, 673228800, 690336000, 667094400, 685843200, 658022400,\n",
      "         675993600, 684374400, 647568000, 706492800, 661219200, 713145600,\n",
      "         640569600, 707702400, 678672000, 671500800, 650332800, 648604800,\n",
      "         709603200, 645062400, 711072000, 644716800, 710553600, 690595200,\n",
      "         647049600, 651542400, 654393600, 652060800, 688089600, 648432000,\n",
      "         712800000, 668908800, 701913600, 715737600, 642297600, 640915200,\n",
      "         706320000, 692150400, 710985600, 654307200]])}, 'needs_exact': tensor([True])} batch, compute_features\n",
      "label_indices tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "        113, 115, 117, 119, 121, 123, 125, 127])\n",
      "normalized_ages tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204])\n",
      "before in norm torch.Size([128, 64])\n",
      "after in norm torch.Size([128, 64])\n",
      "pos_embed torch.Size([128, 1, 8]) torch.Size([128, 1, 8])\n",
      "x in each layer torch.Size([128, 64])\n",
      "x in each layer torch.Size([128, 64])\n",
      "final torch.Size([128, 64])\n",
      "FEMRModel label_indices tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "        113, 115, 117, 119, 121, 123, 125, 127])\n",
      "FEMRModel features torch.Size([128, 64])\n",
      "all_patient_ids torch.Size([64]) feature_times (64,) representations torch.Size([64, 64])\n",
      "{'num_patients': tensor([64]), 'num_indices': tensor([64]), 'patient_ids': tensor([[162, 162, 118, 118, 148, 148, 178, 178,  78,  78,  47,  47,  96,  96,\n",
      "          63,  63, 110, 110,  80,  80,  98,  98,  32,  32, 161, 161, 111, 111,\n",
      "         164, 164, 169, 169,   7,   7,  53,  53, 137, 137, 154, 154, 175, 175,\n",
      "          81,  81,   4,   4,  24,  24, 132, 132,  56,  56, 150, 150,  91,  91,\n",
      "         151, 151,  60,  60, 171, 171, 100, 100, 136, 136,  66,  66,  25,  25,\n",
      "          76,  76, 101, 101, 127, 127,   2,   2, 125, 125,  38,  38,   1,   1,\n",
      "         103, 103, 115, 115, 163, 163,  61,  61,  21,  21,  84,  84, 133, 133,\n",
      "          94,  94,  15,  15,  52,  52, 152, 152, 193, 193, 104, 104, 131, 131,\n",
      "         176, 176, 138, 138, 146, 146, 156, 156,  51,  51, 160, 160, 144, 144,\n",
      "         119, 119]]), 'offsets': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'transformer': {'ages': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]]), 'hierarchical_tokens': tensor([[0, 3, 0, 2, 1, 3, 0, 2, 0, 2, 1, 2, 1, 2, 1, 2, 0, 3, 1, 2, 1, 3, 1, 3,\n",
      "         0, 3, 1, 2, 0, 3, 1, 3, 1, 3, 1, 3, 0, 2, 0, 3, 0, 3, 1, 3, 0, 3, 1, 2,\n",
      "         1, 3, 0, 3, 0, 2, 0, 2, 1, 3, 1, 3, 0, 3, 1, 2, 1, 3, 1, 3, 0, 2, 0, 3,\n",
      "         1, 3, 0, 2, 0, 2, 1, 2, 0, 3, 0, 3, 1, 3, 1, 3, 0, 2, 1, 3, 0, 3, 0, 2,\n",
      "         0, 3, 1, 2, 1, 2, 0, 2, 0, 2, 1, 2, 0, 2, 1, 3, 1, 2, 1, 2, 0, 2, 1, 3,\n",
      "         0, 3, 1, 2, 1, 2, 0, 3]]), 'hierarchical_weights': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]]), 'label_indices': tensor([[  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "          29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "          57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "          85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "         113, 115, 117, 119, 121, 123, 125, 127]]), 'normalized_ages': tensor([[-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204]]), 'patient_lengths': array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]), 'timestamps': tensor([[662601600, 662601600, 663292800, 663292800, 710208000, 710208000,\n",
      "         659318400, 659318400, 646444800, 646444800, 699926400, 699926400,\n",
      "         651024000, 651024000, 653616000, 653616000, 685670400, 685670400,\n",
      "         658195200, 658195200, 671587200, 671587200, 640915200, 640915200,\n",
      "         668476800, 668476800, 642470400, 642470400, 686620800, 686620800,\n",
      "         652233600, 652233600, 717033600, 717033600, 674179200, 674179200,\n",
      "         708998400, 708998400, 641779200, 641779200, 681177600, 681177600,\n",
      "         697593600, 697593600, 714700800, 714700800, 676252800, 676252800,\n",
      "         683078400, 683078400, 696384000, 696384000, 681177600, 681177600,\n",
      "         641260800, 641260800, 694828800, 694828800, 696902400, 696902400,\n",
      "         673660800, 673660800, 711158400, 711158400, 708134400, 708134400,\n",
      "         672969600, 672969600, 650160000, 650160000, 700358400, 700358400,\n",
      "         714268800, 714268800, 670032000, 670032000, 705628800, 705628800,\n",
      "         683769600, 683769600, 697507200, 697507200, 657763200, 657763200,\n",
      "         656294400, 656294400, 657849600, 657849600, 716256000, 716256000,\n",
      "         641952000, 641952000, 641952000, 641952000, 663724800, 663724800,\n",
      "         701481600, 701481600, 701308800, 701308800, 686707200, 686707200,\n",
      "         649641600, 649641600, 702777600, 702777600, 709257600, 709257600,\n",
      "         645408000, 645408000, 702864000, 702864000, 661046400, 661046400,\n",
      "         694656000, 694656000, 696816000, 696816000, 649814400, 649814400,\n",
      "         691286400, 691286400, 685843200, 685843200, 650073600, 650073600,\n",
      "         644976000, 644976000]]), 'token_indices': tensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "         126, 127, 128]]), 'valid_tokens': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True]])}, 'task': {'patient_ids': tensor([[162, 118, 148, 178,  78,  47,  96,  63, 110,  80,  98,  32, 161, 111,\n",
      "         164, 169,   7,  53, 137, 154, 175,  81,   4,  24, 132,  56, 150,  91,\n",
      "         151,  60, 171, 100, 136,  66,  25,  76, 101, 127,   2, 125,  38,   1,\n",
      "         103, 115, 163,  61,  21,  84, 133,  94,  15,  52, 152, 193, 104, 131,\n",
      "         176, 138, 146, 156,  51, 160, 144, 119]]), 'prediction_timestamps': tensor([[662601600, 663292800, 710208000, 659318400, 646444800, 699926400,\n",
      "         651024000, 653616000, 685670400, 658195200, 671587200, 640915200,\n",
      "         668476800, 642470400, 686620800, 652233600, 717033600, 674179200,\n",
      "         708998400, 641779200, 681177600, 697593600, 714700800, 676252800,\n",
      "         683078400, 696384000, 681177600, 641260800, 694828800, 696902400,\n",
      "         673660800, 711158400, 708134400, 672969600, 650160000, 700358400,\n",
      "         714268800, 670032000, 705628800, 683769600, 697507200, 657763200,\n",
      "         656294400, 657849600, 716256000, 641952000, 641952000, 663724800,\n",
      "         701481600, 701308800, 686707200, 649641600, 702777600, 709257600,\n",
      "         645408000, 702864000, 661046400, 694656000, 696816000, 649814400,\n",
      "         691286400, 685843200, 650073600, 644976000]])}, 'needs_exact': tensor([True])} batch, compute_features\n",
      "label_indices tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "        113, 115, 117, 119, 121, 123, 125, 127])\n",
      "normalized_ages tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204])\n",
      "before in norm torch.Size([128, 64])\n",
      "after in norm torch.Size([128, 64])\n",
      "pos_embed torch.Size([128, 1, 8]) torch.Size([128, 1, 8])\n",
      "x in each layer torch.Size([128, 64])\n",
      "x in each layer torch.Size([128, 64])\n",
      "final torch.Size([128, 64])\n",
      "FEMRModel label_indices tensor([  1,   3,   5,   7,   9,  11,  13,  15,  17,  19,  21,  23,  25,  27,\n",
      "         29,  31,  33,  35,  37,  39,  41,  43,  45,  47,  49,  51,  53,  55,\n",
      "         57,  59,  61,  63,  65,  67,  69,  71,  73,  75,  77,  79,  81,  83,\n",
      "         85,  87,  89,  91,  93,  95,  97,  99, 101, 103, 105, 107, 109, 111,\n",
      "        113, 115, 117, 119, 121, 123, 125, 127])\n",
      "FEMRModel features torch.Size([128, 64])\n",
      "all_patient_ids torch.Size([64]) feature_times (64,) representations torch.Size([64, 64])\n",
      "{'num_patients': tensor([8]), 'num_indices': tensor([8]), 'patient_ids': tensor([[196, 196, 183, 183,  46,  46, 145, 145,  16,  16,  31,  31, 157, 157,\n",
      "          11,  11]]), 'offsets': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'transformer': {'ages': tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), 'hierarchical_tokens': tensor([[1, 3, 0, 2, 1, 3, 0, 3, 0, 2, 0, 2, 0, 3, 1, 2]]), 'hierarchical_weights': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]), 'label_indices': tensor([[ 1,  3,  5,  7,  9, 11, 13, 15]]), 'normalized_ages': tensor([[-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "         -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204]]), 'patient_lengths': array([[2, 2, 2, 2, 2, 2, 2, 2]]), 'timestamps': tensor([[703814400, 703814400, 666144000, 666144000, 680832000, 680832000,\n",
      "         691804800, 691804800, 702950400, 702950400, 686534400, 686534400,\n",
      "         658195200, 658195200, 707270400, 707270400]]), 'token_indices': tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]]), 'valid_tokens': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True]])}, 'task': {'patient_ids': tensor([[196, 183,  46, 145,  16,  31, 157,  11]]), 'prediction_timestamps': tensor([[703814400, 666144000, 680832000, 691804800, 702950400, 686534400,\n",
      "         658195200, 707270400]])}, 'needs_exact': tensor([True])} batch, compute_features\n",
      "label_indices tensor([ 1,  3,  5,  7,  9, 11, 13, 15])\n",
      "normalized_ages tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204])\n",
      "before in norm torch.Size([16, 64])\n",
      "after in norm torch.Size([16, 64])\n",
      "pos_embed torch.Size([16, 1, 8]) torch.Size([16, 1, 8])\n",
      "x in each layer torch.Size([16, 64])\n",
      "x in each layer torch.Size([16, 64])\n",
      "final torch.Size([16, 64])\n",
      "FEMRModel label_indices tensor([ 1,  3,  5,  7,  9, 11, 13, 15])\n",
      "FEMRModel features torch.Size([16, 64])\n",
      "all_patient_ids torch.Size([8]) feature_times (8,) representations torch.Size([8, 64])\n",
      "patient_ids (200,)\n",
      "feature_times (200,)\n",
      "features (200, 64)\n"
     ]
    }
   ],
   "source": [
    "import femr.models.transformer\n",
    "import pyarrow.csv\n",
    "import datasets\n",
    "import pickle\n",
    "\n",
    "# First, we compute our features\n",
    "\n",
    "# Load some labels\n",
    "labels = pyarrow.csv.read_csv('input/labels.csv').to_pylist()\n",
    "\n",
    "# Load our data\n",
    "dataset = datasets.Dataset.from_parquet(\"input/meds/data/*\")\n",
    "\n",
    "# We need an ontology for MOTOR\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "features = femr.models.transformer.compute_features(dataset,'input/motor_model', labels, num_proc=4, tokens_per_batch=128, ontology=ontology)\n",
    "\n",
    "# We have our features\n",
    "for k, v in features.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c75a9",
   "metadata": {},
   "source": [
    "# Joining features and labels\n",
    "\n",
    "Given a feature set, it's important to be able to join a set of labels to those features.\n",
    "\n",
    "This can be done with femr.featurizers.join_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad882f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolean_values (200,)\n",
      "patient_ids (200,)\n",
      "times (200,)\n",
      "features (200, 64)\n"
     ]
    }
   ],
   "source": [
    "import femr.featurizers\n",
    "\n",
    "features_and_labels = femr.featurizers.join_labels(features, labels)\n",
    "\n",
    "for k, v in features_and_labels.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192ccc8",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "When using a pretrained CLMBR model, we have to be very careful to use the splits used for the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c49417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.splits\n",
    "import numpy as np\n",
    "\n",
    "# We split into a global training and test set\n",
    "split = femr.splits.PatientSplit.load_from_csv('input/motor_model/main_split.csv')\n",
    "\n",
    "train_mask = np.isin(features_and_labels['patient_ids'], split.train_patient_ids)\n",
    "test_mask = np.isin(features_and_labels['patient_ids'], split.test_patient_ids)\n",
    "\n",
    "percent_train = .70\n",
    "X_train, y_train = (\n",
    "    features_and_labels['features'][train_mask],\n",
    "    features_and_labels['boolean_values'][train_mask],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    features_and_labels['features'][test_mask],\n",
    "    features_and_labels['boolean_values'][test_mask],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deca785",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "The generated features can then be used to build your standard models. In this case we construct both logistic regression and XGBoost models and evaluate them.\n",
    "\n",
    "Performance is perfect since our task (predicting gender) is 100% determined by the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5ad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Logistic Regression ----\n",
      "Train:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n",
      "Test:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def run_analysis(title: str, y_train, y_train_proba, y_test, y_test_proba):\n",
    "    print(f\"---- {title} ----\")\n",
    "    print(\"Train:\")\n",
    "    print_metrics(y_train, y_train_proba)\n",
    "    print(\"Test:\")\n",
    "    print_metrics(y_test, y_test_proba)\n",
    "\n",
    "def print_metrics(y_true, y_proba):\n",
    "    y_pred = y_proba > 0.5\n",
    "    auroc = sklearn.metrics.roc_auc_score(y_true, y_proba)\n",
    "    aps = sklearn.metrics.average_precision_score(y_true, y_proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(\"\\tAUROC:\", auroc)\n",
    "    print(\"\\tAPS:\", aps)\n",
    "    print(\"\\tAccuracy:\", accuracy)\n",
    "    print(\"\\tF1 Score:\", f1)\n",
    "\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegressionCV(penalty=\"l2\", solver=\"liblinear\").fit(X_train, y_train)\n",
    "y_train_proba = model.predict_proba(X_train)[::, 1]\n",
    "y_test_proba = model.predict_proba(X_test)[::, 1]\n",
    "run_analysis(\"Logistic Regression\", y_train, y_train_proba, y_test, y_test_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FEMR_ENV",
   "language": "python",
   "name": "femr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
