{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c59464-ef8f-4fb9-a0a6-ebb68475e2a9",
   "metadata": {},
   "source": [
    "# Using CLMBR to generate features and training models on those features\n",
    "\n",
    "We can use a trained CLMBR model to generate features and then use those features in a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe93d59d-f135-46f6-b0a7-2d75d9b18e6f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'FEMR_ENV' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# change your own cache directory so that you don't redownload from huggingface\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = '/share/pi/nigam/projects/zphuo/.cache'\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_5_INSPECT'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)\n",
    "\n",
    "# number of processes to use\n",
    "num_proc = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaa5d63",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import femr.models.transformer\n",
    "import pyarrow.csv\n",
    "import datasets\n",
    "\n",
    "# First, we compute our features\n",
    "\n",
    "label_columns = '12_month_PH'\n",
    "\n",
    "# Load some labels\n",
    "label_csv_subset = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/cohort_0.2.0_master_file_anon_subset.csv'\n",
    "labels_table = pyarrow.csv.read_csv(label_csv_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d741a7-46a2-4760-a369-3efb01afd804",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'FEMR_ENV (Python 3.10.11)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import femr.models.transformer\n",
    "import pyarrow.csv\n",
    "import datasets\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# Load some labels\n",
    "# labels = pyarrow.csv.read_csv('input/labels.csv').to_pylist()\n",
    "label_csv_subset = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/cohort_0.2.0_master_file_anon_subset.csv'\n",
    "labels_table = pyarrow.csv.read_csv(label_csv_subset)\n",
    "\n",
    "import pandas as pd\n",
    "label_df = pd.read_csv(label_csv_subset)\n",
    "label_df = label_df[['patient_id', 'split', ]]\n",
    "label_df.rename(columns={'split': 'split_name'}, inplace=True)\n",
    "inspect_split_csv = '/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/main_split.csv'\n",
    "label_df.to_csv(inspect_split_csv, index=False)\n",
    "\n",
    "# filter out censored\n",
    "selected_table = labels_table.select(['patient_id', 'procedure_time', label_columns])\n",
    "filtered_table = selected_table.filter(pa.compute.field(label_columns) != \"Censored\")\n",
    "\n",
    "# cast to bool\n",
    "casted_column = pc.cast(filtered_table.column(label_columns), target_type=pa.bool_())\n",
    "filtered_table = filtered_table.set_column(filtered_table.schema.get_field_index(label_columns), pa.field(label_columns, pa.bool_()), casted_column)\n",
    "\n",
    "columns = {name: filtered_table.column(name) for name in filtered_table.column_names}\n",
    "columns['prediction_time'] = columns.pop('procedure_time')\n",
    "columns['boolean_value'] = columns.pop(label_columns)\n",
    "filtered_table = pa.Table.from_arrays(list(columns.values()), names=list(columns.keys()))\n",
    "\n",
    "labels = filtered_table.to_pylist()\n",
    "\n",
    "# Load our data\n",
    "# dataset = datasets.Dataset.from_parquet(\"input/meds/data/*\")\n",
    "parquet_folder = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/data_subset/*'\n",
    "dataset = datasets.Dataset.from_parquet(parquet_folder)\n",
    "\n",
    "model_name = \"StanfordShahLab/clmbr-t-base\"\n",
    "\n",
    "features = femr.models.transformer.compute_features(dataset, model_name, labels, num_proc=num_proc, tokens_per_batch=128)\n",
    "\n",
    "# We have our features\n",
    "for k, v in features.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c75a9",
   "metadata": {},
   "source": [
    "# Joining features and labels\n",
    "\n",
    "Given a feature set, it's important to be able to join a set of labels to those features.\n",
    "\n",
    "This can be done with femr.featurizers.join_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad882f7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolean_values (200,)\n",
      "patient_ids (200,)\n",
      "times (200,)\n",
      "features (200, 64)\n"
     ]
    }
   ],
   "source": [
    "import femr.featurizers\n",
    "\n",
    "features_and_labels = femr.featurizers.join_labels(features, labels)\n",
    "\n",
    "for k, v in features_and_labels.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192ccc8",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "When using a pretrained CLMBR model, we have to be very careful to use the splits used for the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c49417",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import femr.splits\n",
    "import numpy as np\n",
    "\n",
    "# We split into a global training and test set\n",
    "split = femr.splits.PatientSplit.load_from_csv('input/clmbr_model/main_split.csv')\n",
    "\n",
    "train_mask = np.isin(features_and_labels['patient_ids'], split.train_patient_ids)\n",
    "test_mask = np.isin(features_and_labels['patient_ids'], split.test_patient_ids)\n",
    "\n",
    "percent_train = .70\n",
    "X_train, y_train = (\n",
    "    features_and_labels['features'][train_mask],\n",
    "    features_and_labels['boolean_values'][train_mask],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    features_and_labels['features'][test_mask],\n",
    "    features_and_labels['boolean_values'][test_mask],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deca785",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "The generated features can then be used to build your standard models. In this case we construct both logistic regression and XGBoost models and evaluate them.\n",
    "\n",
    "Performance is perfect since our task (predicting gender) is 100% determined by the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad5ad4f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Logistic Regression ----\n",
      "Train:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n",
      "Test:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def run_analysis(title: str, y_train, y_train_proba, y_test, y_test_proba):\n",
    "    print(f\"---- {title} ----\")\n",
    "    print(\"Train:\")\n",
    "    print_metrics(y_train, y_train_proba)\n",
    "    print(\"Test:\")\n",
    "    print_metrics(y_test, y_test_proba)\n",
    "\n",
    "def print_metrics(y_true, y_proba):\n",
    "    y_pred = y_proba > 0.5\n",
    "    auroc = sklearn.metrics.roc_auc_score(y_true, y_proba)\n",
    "    aps = sklearn.metrics.average_precision_score(y_true, y_proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(\"\\tAUROC:\", auroc)\n",
    "    print(\"\\tAPS:\", aps)\n",
    "    print(\"\\tAccuracy:\", accuracy)\n",
    "    print(\"\\tF1 Score:\", f1)\n",
    "\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegressionCV(penalty=\"l2\", solver=\"liblinear\").fit(X_train, y_train)\n",
    "y_train_proba = model.predict_proba(X_train)[::, 1]\n",
    "y_test_proba = model.predict_proba(X_test)[::, 1]\n",
    "run_analysis(\"Logistic Regression\", y_train, y_train_proba, y_test, y_test_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FEMR_ENV",
   "language": "python",
   "name": "femr_env"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
