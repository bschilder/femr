{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0506c9-2c7b-4ccb-851b-993e97c4d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.7\"\n",
    "os.environ[\"JAX_NUMPY_RANK_PROMOTION\"] = \"raise\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a718fda4-1861-4b3c-9cd7-8e6fbd79911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'clmbr'\n",
    "step = '110000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbaf553-3d6e-4875-956b-4bff1e0590cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKING WITH 0.001\n",
      "Starting to load ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import piton.extension.dataloader\n",
    "import msgpack\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"/local-scratch/nigam/projects/ethanid/piton_1_extract\"\n",
    "\n",
    "dictionary_path = (\n",
    "    \"/local-scratch/nigam/projects/ethanid/piton/native/results/dictionary\"\n",
    ")\n",
    "\n",
    "surv_dictionary_path = (\n",
    "    \"/local-scratch/nigam/projects/ethanid/piton/native/results/survival_clmbr_dictionary\"\n",
    ")\n",
    "\n",
    "import piton.datasets\n",
    "\n",
    "data = piton.datasets.PatientDatabase(data_path)\n",
    "male_code = data.get_code_dictionary().index(\"Gender/M\")\n",
    "\n",
    "import json\n",
    "\n",
    "dictionary = msgpack.load(open(dictionary_path, 'rb'), use_list=False)\n",
    "\n",
    "if t == 'survival_clmbr':\n",
    "    surv_dict = msgpack.load(open(surv_dictionary_path, 'rb'), use_list=False)\n",
    "    print(surv_dict.keys())\n",
    "    task = {\"type\": \"survival_clmbr\", \"survival_dict\": surv_dict, \"dim\": 256}\n",
    "elif t == 'clmbr':\n",
    "    task = {\"type\": \"clmbr\", 'vocab_size': 10_000}\n",
    "else:\n",
    "    labels = []\n",
    "\n",
    "    if False:\n",
    "        limit = 100\n",
    "    else:\n",
    "        limit = len(data)\n",
    "\n",
    "    for patient_id in range(0, limit):\n",
    "        patient = data[patient_id]\n",
    "        is_male = any(event.code == male_code for event in patient.events)\n",
    "        labels.append((patient.patient_id, 1, is_male))\n",
    "    task = {\"type\": \"binary\", \"labels\": labels}\n",
    "\n",
    "config = {\n",
    "    \"transformer\": {\n",
    "        \"vocab_size\": 50000,\n",
    "        \"dictionary\": dictionary,\n",
    "        \"hidden_size\": 768,\n",
    "        \"intermediate_size\": 3072,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_layers\": 6,\n",
    "    \n",
    "        \"rotary\": \"per_head\",\n",
    "        \n",
    "        \"max_size\": 13,\n",
    "        \"min_size\": 5,\n",
    "        \n",
    "        \"attention_width\": 256,\n",
    "    },\n",
    "    \n",
    "\n",
    "    \"task\": task,\n",
    "    \"seed\": 97,\n",
    "    \"splits\": [[\"train\", 0, 70], [\"dev\", 70, 85], [\"test\", 85, 100]],\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"l2\": 0,\n",
    "\n",
    "    \"n_epochs\": 100,\n",
    "}\n",
    "\n",
    "print('WORKING WITH', config['learning_rate'])\n",
    "\n",
    "\n",
    "with open(\"trash/config.json\", \"bw\") as f:\n",
    "    msgpack.dump(config, f)\n",
    "\n",
    "loader = piton.extension.dataloader.BatchCreator(data_path, \"trash/config.json\")\n",
    "\n",
    "print(\"Starting to load ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ca3195-2109-4bbc-8e95-e171e080c141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1550\n",
      "340\n",
      "27412\n"
     ]
    }
   ],
   "source": [
    "print(loader.get_number_of_batches(\"train\"))\n",
    "print(loader.get_number_of_batches(\"dev\"))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37990444-a245-48f4-bc2e-83a9e70586ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_patients': 1, 'patient_ids': array([24901,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0], dtype=uint32), 'transformer': {'length': 8192, 'tokens': array([   1,    5,  245, ..., 2218,  106,  950], dtype=uint32), 'ages': array([0.0000000e+00, 9.9930555e-01, 9.9930555e-01, ..., 2.8234000e+04,\n",
      "       2.8234000e+04, 2.8256580e+04], dtype=float32), 'normalized_ages': array([-1.5913894, -1.5912791, -1.5912791, ...,  1.522721 ,  1.522721 ,\n",
      "        1.5252116], dtype=float32), 'label_indices': array([   3,    4,    5, ..., 8192, 8192, 8192], dtype=uint32)}, 'task': {'labels': array([0, 3, 9, ..., 0, 0, 0], dtype=uint32)}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2327"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loader.get_batch(\"train\", 0))\n",
    "len(data[18416].events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc56275-93f3-488b-b85f-7bc2b277f799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch info {'num_patients': ((), dtype('int32')), 'patient_ids': ((256,), dtype('uint32')), 'task': {'labels': ((16384,), dtype('uint32'))}, 'transformer': {'ages': ((8192,), dtype('float32')), 'label_indices': ((16384,), dtype('uint32')), 'length': ((), dtype('int32')), 'normalized_ages': ((8192,), dtype('float32')), 'tokens': ((8192,), dtype('uint32'))}}\n",
      "Compiling the transformer ... (8192,) (16384,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local-scratch/nigam/projects/ethanid/piton/src/piton/jax/__init__.py:677: RuntimeWarning: Using an inefficient CUDA attention mechanism\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params info {'EHRTransformer/~/CLMBRTask/~/linear': {'b': (10000,), 'w': (768, 10000)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': (50000, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_1/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_1/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_1/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_2/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_2/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_2/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_3/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_3/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_3/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_4/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_4/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_4/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_5/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_5/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_5/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}}\n"
     ]
    }
   ],
   "source": [
    "import piton.models.transformer\n",
    "import jax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def roberta_classification_fn(batch):\n",
    "    model = piton.models.transformer.EHRTransformer(config)(batch)\n",
    "    return model\n",
    "\n",
    "\n",
    "dummy_batch=jax.tree_map(lambda a: jnp.array(a), loader.get_batch(\"train\", 2))\n",
    "print(\"Batch info\", jax.tree_map(lambda a: (a.shape, a.dtype), dummy_batch))\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "roberta_classifier = hk.transform(roberta_classification_fn)\n",
    "\n",
    "random_params = roberta_classifier.init(\n",
    "    rng,\n",
    "    batch=dummy_batch,\n",
    ")\n",
    "\n",
    "print(\"Params info\", jax.tree_map(lambda a: a.shape, random_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115ef43a-a934-4f03-a8b8-322131c6196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded params info {'EHRTransformer/~/CLMBRTask/~/linear': {'b': (10000,), 'w': (768, 10000)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/embed': {'embeddings': (50000, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_0/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_1/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_1/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_1/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_2/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_2/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_2/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_3/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_3/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_3/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_4/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_4/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_4/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_5/TransformerBlock/~/layer_norm': {'offset': (768,), 'scale': (768,)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_5/TransformerBlock/~/linear': {'b': (5376,), 'w': (768, 5376)}, 'EHRTransformer/~/TransformerFeaturizer/~/Transformer/~/loop_5/TransformerBlock/~/linear_1': {'b': (768,), 'w': (3840, 768)}}\n"
     ]
    }
   ],
   "source": [
    "loaded_params = pickle.load(open(f\"../native/result_clmbr/params_{t}_{step}\", \"rb\"))\n",
    "\n",
    "print(\"Loaded params info\", jax.tree_map(lambda a: a.shape, loaded_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2b7cf68-03d0-47bc-a0c1-ca51e7ab524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeVar\n",
    "\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "\n",
    "def _cast_floating_to(tree: T, dtype: jnp.dtype) -> T:\n",
    "    def conditional_cast(x):\n",
    "        if isinstance(x, (np.ndarray, jnp.ndarray)) and jnp.issubdtype(\n",
    "            x.dtype, jnp.floating\n",
    "        ):\n",
    "            x = x.astype(dtype)\n",
    "        return x\n",
    "\n",
    "    return jax.tree_util.tree_map(conditional_cast, tree)\n",
    "\n",
    "@jax.jit\n",
    "def compute_loss(params, rng, batch):\n",
    "    loss, logits = roberta_classifier.apply(params, rng, batch)\n",
    "    return loss, logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7470c1e4-472c-4076-a6ef-d7d2a599a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_loss(split, params, rng):\n",
    "    total_loss = 0\n",
    "    num_batches = min(1000, loader.get_number_of_batches(split))\n",
    "    for i in range(num_batches):\n",
    "        batch = loader.get_batch(split, i)\n",
    "        total_loss += compute_loss(_cast_floating_to(params, jnp.float16), rng, batch)[0]\n",
    "        if total_loss != total_loss:\n",
    "            print(\"WAT\", i)\n",
    "            print(1/0)\n",
    "\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a31160-f8a8-4bfc-8758-e7628bb5dabc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the transformer ... (8192,) (16384,)\n",
      "Compiling the transformer ... (8192,) (4096,)\n",
      "0.5394797\n",
      "7.135904\n"
     ]
    }
   ],
   "source": [
    "print(compute_total_loss(\"train\", loaded_params, None))\n",
    "print(compute_total_loss(\"dev\", loaded_params, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e24822-41ba-44c6-9c63-862136a6e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5974655\n",
      "9.597678\n"
     ]
    }
   ],
   "source": [
    "print(compute_total_loss(\"train\", random_params, None))\n",
    "print(compute_total_loss(\"dev\", random_params, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e4c8fd4-c966-431a-b59e-859c644630b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = loader.get_batch(\"train\", 132)\n",
    "\n",
    "loss,logits = compute_loss(_cast_floating_to(loaded_params, jnp.float16), None, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b48f6a-ea00-46cd-af27-aff038aa5a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.52960086\n",
      "[   1   13    4    7  955    0 2504   57   64   96]\n",
      "[16702  8296 11652  5169     0     0     0     0     0     0]\n",
      "4\n",
      "dict_keys(['num_patients', 'patient_ids', 'transformer', 'task'])\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "\n",
    "print(batch['transformer']['tokens'][:10])\n",
    "print(batch['patient_ids'][:10])\n",
    "print(batch['num_patients'])\n",
    "print(batch.keys())\n",
    "print(batch['transformer']['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a946f848-fd88-4721-8dda-c89bb0edeb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3    4    5 ... 8192 8192 8192]\n",
      "(16384,)\n"
     ]
    }
   ],
   "source": [
    "print(batch['transformer']['label_indices'])\n",
    "print(batch['transformer']['label_indices'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6676ba54-d4fc-4aa7-bcee-1c5cb0bc7ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'event_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mevent_indices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(jnp\u001b[38;5;241m.\u001b[39mexp2(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_time\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m][:\u001b[38;5;241m12\u001b[39m]))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'event_indices'"
     ]
    }
   ],
   "source": [
    "print(batch['task'].keys())\n",
    "print(batch['task']['event_indices'])\n",
    "print(jnp.exp2(batch['task']['sparse_time'][1][:12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a057a7-dcd0-42ff-ae49-ae7a9d61231a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)\n",
    "a = jax.nn.softmax(logits)\n",
    "a = a.reshape(batch['transformer']['label_indices'].shape[0], -1, 8192)\n",
    "print(a.shape)\n",
    "print(jnp.min(a[:, 5, 10]))\n",
    "\n",
    "0.0001222 * 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6645f494-5d7d-489c-83fa-e1a1c5e55faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([a for a in loaded_params.keys() if 'Survival' in a])\n",
    "\n",
    "print(loaded_params['EHRTransformer/~/SurvivalCLMBRTask'])\n",
    "print(loaded_params['EHRTransformer/~/SurvivalCLMBRTask/~/linear'])\n",
    "\n",
    "print(jnp.var(loaded_params['EHRTransformer/~/SurvivalCLMBRTask']['code_weights']))\n",
    "print(jnp.var(loaded_params['EHRTransformer/~/SurvivalCLMBRTask/~/linear']['w']))\n",
    "print(1/jnp.sqrt(256))\n",
    "print(1/jnp.sqrt(796))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24272c7d-bc2a-401b-a2fb-a9d6c0389b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cast_floating_to(tree: T, dtype: jnp.dtype) -> T:\n",
    "    def conditional_cast(x):\n",
    "        if isinstance(x, (np.ndarray, jnp.ndarray)) and jnp.issubdtype(\n",
    "            x.dtype, jnp.floating\n",
    "        ):\n",
    "            x = x.astype(dtype)\n",
    "        return x\n",
    "\n",
    "    return jax.tree_util.tree_map(conditional_cast, tree)\n",
    "\n",
    "def pad_to_32(val, dim):\n",
    "    remain = jnp.zeros((val.shape[0], dim - val.shape[1]), dtype=val.dtype)\n",
    "    return jnp.concatenate((remain, val), axis=-1)\n",
    "\n",
    "def loss_fn(bs, batch):\n",
    "    a_matrix = jnp.ones(\n",
    "        (batch['transformer']['label_indices'].shape[0] * len(config['task']['survival_dict']['time_bins']), 1), dtype=bs.dtype)\n",
    "    b_matrix = bs.reshape(-1, 1)\n",
    "    \n",
    "    a_matrix = pad_to_32(a_matrix, 32)\n",
    "    b_matrix = pad_to_32(b_matrix, 32)\n",
    "\n",
    "    survival_loss = piton.jax.exp_mean(\n",
    "        a_matrix, b_matrix, batch['task'][\"sparse_time\"]\n",
    "    )\n",
    "    \n",
    "    event_loss = jnp.log(2) * piton.jax.embedding_dot(\n",
    "        a_matrix, b_matrix, batch['task'][\"event_indices\"]\n",
    "    ).sum(dtype=jnp.float32)\n",
    "    \n",
    "    event_loss = -event_loss / (\n",
    "        a_matrix.shape[0] * b_matrix.shape[0]\n",
    "    )\n",
    "    \n",
    "    return survival_loss + event_loss\n",
    "\n",
    "@jax.value_and_grad\n",
    "def loss_value_and_grad(params, loss_scale, batch):\n",
    "    loss = loss_fn(params, batch)\n",
    "\n",
    "    assert loss.dtype == jnp.float32\n",
    "\n",
    "    post_scale = loss_scale.scale(loss)\n",
    "    return post_scale\n",
    "\n",
    "def apply_optimizer(params, grads, opt_state):\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state\n",
    "\n",
    "@jax.jit\n",
    "def update(params, loss_scale, opt_state, batch):\n",
    "    batch_loss, grads = loss_value_and_grad(\n",
    "        _cast_floating_to(params, jnp.float16), loss_scale, batch\n",
    "    )\n",
    "\n",
    "    batch_loss = loss_scale.unscale(batch_loss.astype(jnp.float32))\n",
    "    grads = loss_scale.unscale(_cast_floating_to(grads, jnp.float32))\n",
    "\n",
    "    grads_finite = jmp.all_finite(grads)\n",
    "\n",
    "    loss_scale = loss_scale.adjust(grads_finite)\n",
    "\n",
    "    new_params, opt_state = jmp.select_tree(\n",
    "        grads_finite,\n",
    "        apply_optimizer(params, grads, opt_state),\n",
    "        (params, opt_state),\n",
    "    )\n",
    "\n",
    "    return new_params, opt_state, batch_loss, loss_scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916fa77-e951-45ed-9c24-b1773e4645b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import jmp\n",
    "\n",
    "\n",
    "\n",
    "params = jax.random.normal(jax.random.PRNGKey(123), \n",
    "                           shape=(len(config['task']['survival_dict']['codes']),)) - 13\n",
    "\n",
    "opt = optax.sgd(learning_rate=1e-2)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "loss_scale = jmp.DynamicLossScale(jnp.array(2**15, dtype=jnp.float32))\n",
    "\n",
    "for i in range(loader.get_number_of_batches(\"train\")):\n",
    "    batch = loader.get_batch(\"train\", i)\n",
    "    batch = jax.tree_map(lambda a: jnp.array(a), batch)\n",
    "    params, opt_state, batch_loss, loss_scale = update(\n",
    "        params, loss_scale, opt_state, batch)\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss_scale, params, batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82307c36-c6b1-4c9d-b5a0-1a62d9e5f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import jmp\n",
    "\n",
    "\n",
    "params = jnp.log2(jnp.array(config['task']['survival_dict']['lambdas']))\n",
    "print(params)\n",
    "\n",
    "opt = optax.adam(learning_rate=1e-2)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "loss_scale = jmp.DynamicLossScale(jnp.array(2**15, dtype=jnp.float32))\n",
    "\n",
    "for i in range(loader.get_number_of_batches(\"train\")):\n",
    "    batch = loader.get_batch(\"train\", i)\n",
    "    batch = jax.tree_map(lambda a: jnp.array(a), batch)\n",
    "    params, opt_state, batch_loss, loss_scale = update(\n",
    "        params, loss_scale, opt_state, batch)\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss_scale, params, batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe011c8d-209a-4708-92c3-6c50894c0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "theoretically_optimal_params = copy.deepcopy(loaded_params)\n",
    "print(theoretically_optimal_params.keys())\n",
    "\n",
    "\n",
    "print(compute_total_loss(\"dev\", theoretically_optimal_params, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc10c38-97c0-4d00-83ab-f2e53c75b870",
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretically_optimal_params['EHRTransformer/~/SurvivalCLMBRTask']['code_weights'] = jnp.zeros_like(theoretically_optimal_params['EHRTransformer/~/SurvivalCLMBRTask']['code_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31d3fa-e4e8-4f73-8094-ceb1d58b6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_total_loss(\"dev\", theoretically_optimal_params, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1872a34-dc0f-4102-83ef-9043bd598bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theoretically_optimal_params['EHRTransformer/~/SurvivalCLMBRTask']['code_weights'].shape)\n",
    "\n",
    "better = pad_to_32(jnp.log2(jnp.array(config['task']['survival_dict']['lambdas'])).reshape(-1, 1), dim=256)\n",
    "\n",
    "print(better)\n",
    "\n",
    "theoretically_optimal_params['EHRTransformer/~/SurvivalCLMBRTask']['code_weights'] = better\n",
    "\n",
    "print(compute_total_loss(\"dev\", theoretically_optimal_params, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5da18e-49fc-4e5a-aeea-b4eeb4436e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "better = pad_to_32(params.reshape(-1, 1), dim=256)\n",
    "\n",
    "print(better)\n",
    "\n",
    "theoretically_optimal_params['EHRTransformer/~/SurvivalCLMBRTask']['code_weights'] = better\n",
    "\n",
    "print(compute_total_loss(\"dev\", theoretically_optimal_params, None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
