{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running CLMBR\n",
    "\n",
    "CLMBR is a model that can predict future patient events given past events. The tools to run this also depend on some CLI tools that come from `pip install femr`.\n",
    "\n",
    "The general approach is:\n",
    "1. Create a dictionary\n",
    "2. Create batches of patient data\n",
    "3. Train the model\n",
    "4. Compute representations\n",
    "5. Use the model\n",
    "\n",
    "The commands are as-follows:\n",
    "1. `clmbr_create_dictionary`\n",
    "    - `clmbr_create_survival_dictionary`\n",
    "2. `clmbr_create_batches`\n",
    "3. `clmbr_train_model`\n",
    "4. `clmbr_compute_representations`\n",
    "5. Reference the model directory in the code\n",
    "\n",
    "\n",
    "To use CLMBR, you'll need have CUDA on Linux.\n",
    "\n",
    "You'll need to install some extra packages:\n",
    "```bash\n",
    "# clmbr_create_dictionary\n",
    "pip install dm-haiku optax\n",
    "\n",
    "# clmbr_create_batches\n",
    "pip install jax\n",
    "pip install --upgrade \"jax[cpu]\"\n",
    "\n",
    "# clmbr_train_model, clmbr_compute_representations\n",
    "# If using CUDA on Linux\n",
    "pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banned 0 out of 10\n",
      "Got age statistics ... {\"mean\":23346308.57142857,\"std\":9540706.023008432}\n"
     ]
    }
   ],
   "source": [
    "!clmbr_create_dictionary --data_path ./example_data/example_etl_output ./example_data/dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-18 11:03:35,670 [MainThread  ] [INFO ]  Preparing batches with Namespace(directory='./example_data/batches', data_path='./example_data/example_etl_output', dictionary_path='./example_data/dictionary', task='clmbr', transformer_vocab_size=5, clmbr_survival_dictionary_path=None, labeled_patients_path=None, is_hierarchical=False, seed=97, val_start=70, test_start=85, batch_size=16384, note_embedding_data=None, limit_to_patients_file=None, limit_before_date=None)\n",
      "2023-05-18 11:03:35,671 [MainThread  ] [INFO ]  Wrote config ...\n",
      "2023-05-18 11:03:35,671 [MainThread  ] [INFO ]  Starting to load\n",
      "When mapping codes, dropped 0 out of 5\n",
      "2023-05-18 11:03:35,672 [MainThread  ] [INFO ]  Loaded\n",
      "When mapping codes, dropped 0 out of 5\n",
      "2023-05-18 11:03:35,673 [MainThread  ] [INFO ]  Number of train patients 1\n"
     ]
    }
   ],
   "source": [
    "# `--transformer_vocab_size`` needs to have a value less than number of codes in the dictionary.\n",
    "!clmbr_create_batches --data_path ./example_data/example_etl_output --dictionary_path ./example_data/dictionary --transformer_vocab_size 5 --task clmbr ./example_data/batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-18 10:51:11,403 [MainThread  ] [INFO ]  Training model with Namespace(directory='./example_data/trained_model', data_path='./example_data/example_etl_output', batches_path='./example_data/batches', learning_rate=0.1, rotary_type='disabled', clmbr_survival_dim=None, num_batch_threads=None, start_from_checkpoint=None, freeze_weights=False, token_dropout=0, internal_dropout=0, weight_decay=0, max_iter=None, hidden_size=768, intermediate_size=3072, n_heads=12, n_layers=6, attention_width=512, dev_batches_path=None, early_stopping_window_steps=None)\n",
      "2023-05-18 10:51:11,404 [MainThread  ] [INFO ]  Got config {'data_path': './example_data/example_etl_output', 'batch_info_path': './example_data/batches/batch_info.msgpack', 'seed': 97, 'task': {'type': 'clmbr', 'vocab_size': 8192}, 'transformer': {'vocab_size': 5, 'hidden_size': 768, 'intermediate_size': 3072, 'n_heads': 12, 'n_layers': 6, 'rotary': 'disabled', 'attention_width': 496, 'internal_dropout': 0, 'is_hierarchical': False, 'note_embedding_data': None}, 'learning_rate': 0.1, 'max_grad_norm': 1.0, 'weight_decay': 0, 'n_epochs': 100}\n",
      "When mapping codes, dropped 0 out of 5\n",
      "2023-05-18 10:51:11,404 [MainThread  ] [INFO ]  Loaded batches 1 0\n",
      "2023-05-18 10:51:11,407 [MainThread  ] [INFO ]  Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2023-05-18 10:51:11,408 [MainThread  ] [INFO ]  Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "2023-05-18 10:51:11,408 [MainThread  ] [INFO ]  Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "2023-05-18 10:51:11,408 [MainThread  ] [INFO ]  Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "2023-05-18 10:51:11,417 [MainThread  ] [INFO ]  Got dummy batch {'num_indices': ((), dtype('int32'), CpuDevice(id=0)), 'num_patients': ((), dtype('int32'), CpuDevice(id=0)), 'offsets': ((512,), dtype('uint32'), CpuDevice(id=0)), 'patient_ids': ((512,), dtype('uint32'), CpuDevice(id=0)), 'task': {'labels': ((256,), dtype('uint32'), CpuDevice(id=0))}, 'transformer': {'ages': ((16384,), dtype('float32'), CpuDevice(id=0)), 'label_indices': ((256,), dtype('uint32'), CpuDevice(id=0)), 'length': ((), dtype('int32'), CpuDevice(id=0)), 'normalized_ages': ((16384,), dtype('float32'), CpuDevice(id=0)), 'tokens': ((16384,), dtype('uint32'), CpuDevice(id=0)), 'valid_tokens': ((16384,), dtype('bool'), CpuDevice(id=0))}}\n",
      "2023-05-18 10:51:11,426 [MainThread  ] [INFO ]  Transformed the model function\n",
      "Compiling the transformer ... (16384,) (256,)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/bin/clmbr_train_model\", line 8, in <module>\n",
      "    sys.exit(train_model())\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/scripts.py\", line 205, in train_model\n",
      "    params = jax.jit(model.init, static_argnames=[\"config\", \"is_training\"])(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/pjit.py\", line 208, in cache_miss\n",
      "    outs, out_flat, out_tree, args_flat = _python_pjit_helper(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/pjit.py\", line 150, in _python_pjit_helper\n",
      "    args_flat, _, params, in_tree, out_tree, _ = infer_params_fn(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/api.py\", line 301, in infer_params\n",
      "    return pjit.common_infer_params(pjit_info_args, *args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/pjit.py\", line 474, in common_infer_params\n",
      "    jaxpr, consts, canonicalized_out_shardings_flat = _pjit_jaxpr(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/pjit.py\", line 935, in _pjit_jaxpr\n",
      "    jaxpr, final_consts, out_type = _create_pjit_jaxpr(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 345, in memoized_fun\n",
      "    ans = call(fun, *args)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/pjit.py\", line 888, in _create_pjit_jaxpr\n",
      "    jaxpr, global_out_avals, consts = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2150, in trace_to_jaxpr_dynamic\n",
      "    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2172, in trace_to_subjaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers_)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n",
      "    ans = self.f(*args, **dict(self.params, **kwargs))\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 114, in init_fn\n",
      "    params, state = f.init(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 338, in init_fn\n",
      "    f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/scripts.py\", line 189, in model_fn\n",
      "    model = femr.models.transformer.EHRTransformer(config)(batch, is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 508, in __call__\n",
      "    features, mask = self.featurizer(batch[\"transformer\"], is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 279, in __call__\n",
      "    sequence_data = self.transformer(batch, is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 259, in __call__\n",
      "    final_x = jax.lax.scan(process, x, all_stacked_tree)[0]\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py\", line 249, in scan\n",
      "    init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py\", line 235, in _create_jaxpr\n",
      "    jaxpr, consts, out_tree = _initial_style_jaxpr(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/lax/control_flow/common.py\", line 60, in _initial_style_jaxpr\n",
      "    jaxpr, consts, out_tree = _initial_style_open_jaxpr(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/lax/control_flow/common.py\", line 54, in _initial_style_open_jaxpr\n",
      "    jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(wrapped_fun, in_avals, debug)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/profiler.py\", line 314, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2150, in trace_to_jaxpr_dynamic\n",
      "    jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2172, in trace_to_subjaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers_)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n",
      "    ans = self.f(*args, **dict(self.params, **kwargs))\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 256, in process\n",
      "    res = self.layer_transform.apply(params, rng, v, normed_ages, pos_embed, batch, is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 128, in apply_fn\n",
      "    out, state = f.apply(params, {}, *args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 357, in apply_fn\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 168, in <lambda>\n",
      "    self.layer_transform = hk.transform(lambda *args: TransformerBlock(config)(*args))\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 122, in __call__\n",
      "    attn = femr.jax.local_attention(q, k, v, length_mask, self.config[\"attention_width\"])\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 166, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/custom_derivatives.py\", line 614, in __call__\n",
      "    out_flat = custom_vjp_call_p.bind(flat_fun, flat_fwd, flat_bwd,\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/custom_derivatives.py\", line 763, in bind\n",
      "    outs = top_trace.process_custom_vjp_call(self, fun, fwd, bwd_, tracers,\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 1985, in process_custom_vjp_call\n",
      "    fun_jaxpr, out_avals, consts = trace_to_subjaxpr_dynamic(fun, self.main, in_avals)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2172, in trace_to_subjaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers_)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 188, in call_wrapped\n",
      "    ans = self.f(*args, **dict(self.params, **kwargs))\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/jax.py\", line 687, in local_attention\n",
      "    return local_attention_fwd(queries, keys, values, length_mask, attention_width)[0]\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/jax.py\", line 777, in local_attention_fwd\n",
      "    attention, result = local_attention_forward_p.bind(queries, keys, values, length, attention_width=attention_width)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/core.py\", line 380, in bind\n",
      "    return self.bind_with_trace(find_top_trace(args), args, params)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/core.py\", line 383, in bind_with_trace\n",
      "    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 1855, in process_primitive\n",
      "    return self.default_process_primitive(primitive, tracers, params)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 1859, in default_process_primitive\n",
      "    out_avals, effects = primitive.abstract_eval(*avals, **params)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/jax/_src/core.py\", line 416, in abstract_eval_\n",
      "    return abstract_eval(*args, **kwargs), no_effects\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/jax.py\", line 829, in local_attention_forward_abstract_eval\n",
      "    femr.extension.jax.get_local_attention_shape(\n",
      "jax._src.traceback_util.UnfilteredStackTrace: AttributeError: module 'femr.extension.jax' has no attribute 'get_local_attention_shape'\n",
      "\n",
      "The stack trace below excludes JAX-internal frames.\n",
      "The preceding is the original exception that occurred, unmodified.\n",
      "\n",
      "--------------------\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/bin/clmbr_train_model\", line 8, in <module>\n",
      "    sys.exit(train_model())\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/scripts.py\", line 205, in train_model\n",
      "    params = jax.jit(model.init, static_argnames=[\"config\", \"is_training\"])(\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 114, in init_fn\n",
      "    params, state = f.init(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 338, in init_fn\n",
      "    f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/scripts.py\", line 189, in model_fn\n",
      "    model = femr.models.transformer.EHRTransformer(config)(batch, is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 508, in __call__\n",
      "    features, mask = self.featurizer(batch[\"transformer\"], is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 279, in __call__\n",
      "    sequence_data = self.transformer(batch, is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 259, in __call__\n",
      "    final_x = jax.lax.scan(process, x, all_stacked_tree)[0]\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 256, in process\n",
      "    res = self.layer_transform.apply(params, rng, v, normed_ages, pos_embed, batch, is_training)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 128, in apply_fn\n",
      "    out, state = f.apply(params, {}, *args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/transform.py\", line 357, in apply_fn\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 168, in <lambda>\n",
      "    self.layer_transform = hk.transform(lambda *args: TransformerBlock(config)(*args))\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 426, in wrapped\n",
      "    out = f(*args, **kwargs)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/Users/ericpan/anaconda3/envs/FEMR_TEST_1/lib/python3.10/site-packages/haiku/_src/module.py\", line 272, in run_interceptors\n",
      "    return bound_method(*args, **kwargs)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/models/transformer.py\", line 122, in __call__\n",
      "    attn = femr.jax.local_attention(q, k, v, length_mask, self.config[\"attention_width\"])\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/jax.py\", line 687, in local_attention\n",
      "    return local_attention_fwd(queries, keys, values, length_mask, attention_width)[0]\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/jax.py\", line 777, in local_attention_fwd\n",
      "    attention, result = local_attention_forward_p.bind(queries, keys, values, length, attention_width=attention_width)\n",
      "  File \"/Users/ericpan/GitHub/femr/src/femr/jax.py\", line 829, in local_attention_forward_abstract_eval\n",
      "    femr.extension.jax.get_local_attention_shape(\n",
      "AttributeError: module 'femr.extension.jax' has no attribute 'get_local_attention_shape'\n"
     ]
    }
   ],
   "source": [
    "!clmbr_train_model --data_path ./example_data/example_etl_output --batches_path ./example_data/batches --learning_rate 0.1 --rotary_type disabled ./example_data/trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: Train [-h] --data_path DATA_PATH --batches_path BATCHES_PATH\n",
      "             --learning_rate LEARNING_RATE --rotary_type ROTARY_TYPE\n",
      "             [--clmbr_survival_dim CLMBR_SURVIVAL_DIM]\n",
      "             [--num_batch_threads NUM_BATCH_THREADS]\n",
      "             [--start_from_checkpoint START_FROM_CHECKPOINT]\n",
      "             [--freeze_weights] [--token_dropout TOKEN_DROPOUT]\n",
      "             [--internal_dropout INTERNAL_DROPOUT]\n",
      "             [--weight_decay WEIGHT_DECAY] [--max_iter MAX_ITER]\n",
      "             [--hidden_size HIDDEN_SIZE]\n",
      "             [--intermediate_size INTERMEDIATE_SIZE] [--n_heads N_HEADS]\n",
      "             [--n_layers N_LAYERS] [--attention_width ATTENTION_WIDTH]\n",
      "             [--dev_batches_path DEV_BATCHES_PATH]\n",
      "             [--early_stopping_window_steps EARLY_STOPPING_WINDOW_STEPS]\n",
      "             directory\n",
      "\n",
      "positional arguments:\n",
      "  directory\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data_path DATA_PATH\n",
      "  --batches_path BATCHES_PATH\n",
      "  --learning_rate LEARNING_RATE\n",
      "  --rotary_type ROTARY_TYPE\n",
      "  --clmbr_survival_dim CLMBR_SURVIVAL_DIM\n",
      "  --num_batch_threads NUM_BATCH_THREADS\n",
      "  --start_from_checkpoint START_FROM_CHECKPOINT\n",
      "  --freeze_weights\n",
      "  --token_dropout TOKEN_DROPOUT\n",
      "  --internal_dropout INTERNAL_DROPOUT\n",
      "  --weight_decay WEIGHT_DECAY\n",
      "  --max_iter MAX_ITER\n",
      "  --hidden_size HIDDEN_SIZE\n",
      "                        Transformer hidden size\n",
      "  --intermediate_size INTERMEDIATE_SIZE\n",
      "                        Transformer intermediate layer size\n",
      "  --n_heads N_HEADS     Transformer # of heads\n",
      "  --n_layers N_LAYERS   Transformer # of layers\n",
      "  --attention_width ATTENTION_WIDTH\n",
      "                        Transformer attention width.\n",
      "  --dev_batches_path DEV_BATCHES_PATH\n",
      "                        Do early stopping with a different set of batches\n",
      "                        instead of the development set\n",
      "  --early_stopping_window_steps EARLY_STOPPING_WINDOW_STEPS\n",
      "                        If we don't see a decrease in dev loss in this many\n",
      "                        steps, stop training. A reasonable value is 15000.\n"
     ]
    }
   ],
   "source": [
    "!clmbr_train_model --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: Compute representations [-h] --data_path DATA_PATH --batches_path\n",
      "                               BATCHES_PATH --model_dir MODEL_DIR\n",
      "                               destination\n",
      "\n",
      "positional arguments:\n",
      "  destination\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data_path DATA_PATH\n",
      "  --batches_path BATCHES_PATH\n",
      "  --model_dir MODEL_DIR\n"
     ]
    }
   ],
   "source": [
    "!clmbr_compute_representations --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: How to reference trained CLMBR directory in the code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FEMR_TEST_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
