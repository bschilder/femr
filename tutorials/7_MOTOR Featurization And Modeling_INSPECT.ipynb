{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c59464-ef8f-4fb9-a0a6-ebb68475e2a9",
   "metadata": {},
   "source": [
    "# Using CLMBR to generate features and training models on those features\n",
    "\n",
    "We can use a trained CLMBR model to generate features and then use those features in a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe93d59d-f135-46f6-b0a7-2d75d9b18e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_7_INSPECT'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)\n",
    "\n",
    "num_proc = 20"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f2f732a",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def validateJSON(jsonData):\n",
    "    try:\n",
    "        json.loads(jsonData)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def rectifyJSON(jsonFile):\n",
    "    new_json = open(jsonFile.replace('.json', '_new.json'), 'w')\n",
    "    with open(jsonFile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace(\"'\", '\"')\n",
    "            new_json.write(line)\n",
    "    new_json.close()\n",
    "    return jsonFile.replace('.json', '_new.json')\n",
    "\n",
    "\n",
    "# pretrain_model_config = '/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/config.json'\n",
    "# pretrain_model_config = '/share/pi/nigam/ethanid/femrv3/femr/tutorials/trash/tutorial_6/motor_model/config.json'\n",
    "\n",
    "if not validateJSON(pretrain_model_config):\n",
    "    print('Rectifying JSON')\n",
    "    pretrain_model_config = rectifyJSON(pretrain_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d741a7-46a2-4760-a369-3efb01afd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=20): 100%|██████████| 1916/1916 [00:00<00:00, 9063.46 examples/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/main' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m model_folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[39m# model_folder = '/share/pi/nigam/ethanid/femrv2/real_data_model'\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m features \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mcompute_features(dataset, model_folder, labels, num_proc\u001b[39m=\u001b[39;49mnum_proc, tokens_per_batch\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, ontology\u001b[39m=\u001b[39;49montology)\n\u001b[1;32m     49\u001b[0m \u001b[39m# We have our features\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/repos/femr/src/femr/models/transformer.py:389\u001b[0m, in \u001b[0;36mcompute_features\u001b[0;34m(dataset, model_path, labels, num_proc, tokens_per_batch, device, ontology)\u001b[0m\n\u001b[1;32m    386\u001b[0m task \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mtasks\u001b[39m.\u001b[39mLabeledPatientTask(labels)\n\u001b[1;32m    387\u001b[0m index \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mPatientIndex(dataset, num_proc\u001b[39m=\u001b[39mnum_proc)\n\u001b[0;32m--> 389\u001b[0m model \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mFEMRModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_path, task_config\u001b[39m=\u001b[39;49mtask\u001b[39m.\u001b[39;49mget_task_config())\n\u001b[1;32m    390\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(model), \u001b[39m'\u001b[39m\u001b[39mmodel type\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    391\u001b[0m tokenizer \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mFEMRTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_path, ontology\u001b[39m=\u001b[39montology)\n",
      "File \u001b[0;32m~/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/transformers/modeling_utils.py:2981\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2980\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2981\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   2982\u001b[0m         config_path,\n\u001b[1;32m   2983\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2984\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2985\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2986\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2987\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2988\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2989\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2990\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2991\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2992\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   2993\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   2994\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2995\u001b[0m     )\n\u001b[1;32m   2996\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[39m# In case one passes a config to `from_pretrained` + \"attn_implementation\"\u001b[39;00m\n\u001b[1;32m   2998\u001b[0m     \u001b[39m# override the `_attn_implementation` attribute to `attn_implementation` of the kwargs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3002\u001b[0m     \u001b[39m# we pop attn_implementation from the kwargs but this handles the case where users\u001b[39;00m\n\u001b[1;32m   3003\u001b[0m     \u001b[39m# passes manually the config to `from_pretrained`.\u001b[39;00m\n\u001b[1;32m   3004\u001b[0m     config \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/transformers/configuration_utils.py:604\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mrevision\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m revision\n\u001b[1;32m    602\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 604\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    605\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    606\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    607\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    608\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/transformers/configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    632\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    635\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/transformers/configuration_utils.py:688\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    686\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    689\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    690\u001b[0m         configuration_file,\n\u001b[1;32m    691\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    692\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    693\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    694\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    695\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    696\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    697\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    698\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    699\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    700\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    702\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    703\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/transformers/utils/hub.py:369\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    368\u001b[0m     \u001b[39mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 369\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         )\n\u001b[1;32m    373\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /home/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/main' for available files."
     ]
    }
   ],
   "source": [
    "import femr.models.transformer\n",
    "import pyarrow.csv\n",
    "import datasets\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# First, we compute our features\n",
    "\n",
    "# Load some labels\n",
    "# labels = pyarrow.csv.read_csv('input/labels.csv').to_pylist()\n",
    "# label_csv = '/share/pi/nigam/projects/zphuo/data/PE/inspect/cohort_0.2.0_master_file_anon.csv'\n",
    "label_csv_subset = '/home/zphuo/data/timelines_smallfiles_meds/cohort_0.2.0_master_file_anon_subset.csv'\n",
    "\n",
    "labels_table = pyarrow.csv.read_csv(label_csv_subset)\n",
    "\n",
    "# filter out censored\n",
    "selected_table = labels_table.select(['patient_id', 'procedure_time', '12_month_PH'])\n",
    "filtered_table = selected_table.filter(pa.compute.field(\"12_month_PH\") != \"Censored\")\n",
    "\n",
    "# cast to bool\n",
    "casted_column = pc.cast(filtered_table.column('12_month_PH'), target_type=pa.bool_())\n",
    "filtered_table = filtered_table.set_column(filtered_table.schema.get_field_index('12_month_PH'), pa.field('12_month_PH', pa.bool_()), casted_column)\n",
    "\n",
    "\n",
    "columns = {name: filtered_table.column(name) for name in filtered_table.column_names}\n",
    "columns['prediction_time'] = columns.pop('procedure_time')\n",
    "columns['boolean_value'] = columns.pop('12_month_PH')\n",
    "filtered_table = pa.Table.from_arrays(list(columns.values()), names=list(columns.keys()))\n",
    "\n",
    "labels = filtered_table.to_pylist()\n",
    "\n",
    "# Load our data\n",
    "# dataset = datasets.Dataset.from_parquet(\"input/meds/data/*\")\n",
    "parquet_folder = '/home/zphuo/data/timelines_smallfiles_meds/data_subset/*'\n",
    "dataset = datasets.Dataset.from_parquet(parquet_folder)\n",
    "\n",
    "# femr.ontology.Ontology will create one given an athena path and code_metadata\n",
    "# We need an ontology for MOTOR\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "model_folder = '/home/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model'\n",
    "# model_folder = '/share/pi/nigam/ethanid/femrv2/real_data_model'\n",
    "\n",
    "\n",
    "features = femr.models.transformer.compute_features(dataset, model_folder, labels, num_proc=num_proc, tokens_per_batch=128, ontology=ontology)\n",
    "\n",
    "# We have our features\n",
    "for k, v in features.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c75a9",
   "metadata": {},
   "source": [
    "# Joining features and labels\n",
    "\n",
    "Given a feature set, it's important to be able to join a set of labels to those features.\n",
    "\n",
    "This can be done with femr.featurizers.join_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad882f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolean_values (1872,)\n",
      "patient_ids (1872,)\n",
      "times (1872,)\n",
      "features (1872, 768)\n"
     ]
    }
   ],
   "source": [
    "import femr.featurizers\n",
    "\n",
    "features_and_labels = femr.featurizers.join_labels(features, labels)\n",
    "\n",
    "for k, v in features_and_labels.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192ccc8",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "When using a pretrained CLMBR model, we have to be very careful to use the splits used for the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c49417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.splits\n",
    "import numpy as np\n",
    "\n",
    "# We split into a global training and test set\n",
    "split = femr.splits.PatientSplit.load_from_csv('/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/main_split.csv')\n",
    "\n",
    "train_mask = np.isin(features_and_labels['patient_ids'], split.train_patient_ids)\n",
    "test_mask = np.isin(features_and_labels['patient_ids'], split.test_patient_ids)\n",
    "\n",
    "# percent_train = .70\n",
    "X_train, y_train = (\n",
    "    features_and_labels['features'][train_mask],\n",
    "    features_and_labels['boolean_values'][train_mask],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    features_and_labels['features'][test_mask],\n",
    "    features_and_labels['boolean_values'][test_mask],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deca785",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "The generated features can then be used to build your standard models. In this case we construct both logistic regression and XGBoost models and evaluate them.\n",
    "\n",
    "Performance is perfect since our task (predicting gender) is 100% determined by the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad5ad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Logistic Regression ----\n",
      "Train:\n",
      "\tAUROC: 0.5968191769045453\n",
      "\tAPS: 0.18090006751567927\n",
      "\tAccuracy: 0.864375\n",
      "\tF1 Score: 0.0\n",
      "Test:\n",
      "\tAUROC: 0.5588908981314045\n",
      "\tAPS: 0.1792486980822997\n",
      "\tAccuracy: 0.8713235294117647\n",
      "\tF1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def run_analysis(title: str, y_train, y_train_proba, y_test, y_test_proba):\n",
    "    print(f\"---- {title} ----\")\n",
    "    print(\"Train:\")\n",
    "    print_metrics(y_train, y_train_proba)\n",
    "    print(\"Test:\")\n",
    "    print_metrics(y_test, y_test_proba)\n",
    "\n",
    "def print_metrics(y_true, y_proba):\n",
    "    y_pred = y_proba > 0.5\n",
    "    auroc = sklearn.metrics.roc_auc_score(y_true, y_proba)\n",
    "    aps = sklearn.metrics.average_precision_score(y_true, y_proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(\"\\tAUROC:\", auroc)\n",
    "    print(\"\\tAPS:\", aps)\n",
    "    print(\"\\tAccuracy:\", accuracy)\n",
    "    print(\"\\tF1 Score:\", f1)\n",
    "\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegressionCV(penalty=\"l2\", solver=\"liblinear\").fit(X_train, y_train)\n",
    "y_train_proba = model.predict_proba(X_train)[::, 1]\n",
    "y_test_proba = model.predict_proba(X_test)[::, 1]\n",
    "run_analysis(\"Logistic Regression\", y_train, y_train_proba, y_test, y_test_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8f6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FEMR_ENV",
   "language": "python",
   "name": "femr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
