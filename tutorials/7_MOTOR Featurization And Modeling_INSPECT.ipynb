{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c59464-ef8f-4fb9-a0a6-ebb68475e2a9",
   "metadata": {},
   "source": [
    "# Using CLMBR to generate features and training models on those features\n",
    "\n",
    "We can use a trained CLMBR model to generate features and then use those features in a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe93d59d-f135-46f6-b0a7-2d75d9b18e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_7_INSPECT'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)\n",
    "\n",
    "\n",
    "num_proc = 20"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f2f732a",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def validateJSON(jsonData):\n",
    "    try:\n",
    "        json.loads(jsonData)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def rectifyJSON(jsonFile):\n",
    "    new_json = open(jsonFile.replace('.json', '_new.json'), 'w')\n",
    "    with open(jsonFile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace(\"'\", '\"')\n",
    "            new_json.write(line)\n",
    "    new_json.close()\n",
    "    return jsonFile.replace('.json', '_new.json')\n",
    "\n",
    "\n",
    "# pretrain_model_config = '/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model/config.json'\n",
    "# pretrain_model_config = '/share/pi/nigam/ethanid/femrv3/femr/tutorials/trash/tutorial_6/motor_model/config.json'\n",
    "\n",
    "if not validateJSON(pretrain_model_config):\n",
    "    print('Rectifying JSON')\n",
    "    pretrain_model_config = rectifyJSON(pretrain_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d741a7-46a2-4760-a369-3efb01afd804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8872fd8b189948dfa2c231a194429b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model were not used when initializing FEMRModel: ['task_model.final_layer.bias', 'task_model.final_layer.weight', 'task_model.task_layer.bias', 'task_model.task_layer.weight']\n",
      "- This IS expected if you are initializing FEMRModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FEMRModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54684ab6c90463c98d7e742ba870ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/1574 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches 1274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660805dd4a8a4814ba150c9c4fe3a9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/builder.py\", line 1670, in _prepare_split_single\n    for key, record in generator:\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/packaged_modules/generator/generator.py\", line 30, in _generate_examples\n    for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\n  File \"/share/pi/nigam/projects/zphuo/repos/femr/src/femr/models/processor.py\", line 213, in _batch_generator\n    assert \"task\" in result, f\"No task present in {lengths[start:end,:]}\"\nAssertionError: No task present in [[1131    0    1]\n [ 763    0    1]]\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1377, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/builder.py\", line 1706, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m\n\u001b[1;32m     43\u001b[0m model_folder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[39m# model_folder = '/share/pi/nigam/ethanid/femrv3/femr/tutorials/trash/tutorial_6/motor_model'\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m features \u001b[39m=\u001b[39m femr\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mtransformer\u001b[39m.\u001b[39;49mcompute_features(dataset, model_folder, labels, num_proc\u001b[39m=\u001b[39;49mnum_proc, tokens_per_batch\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, ontology\u001b[39m=\u001b[39;49montology)\n\u001b[1;32m     49\u001b[0m \u001b[39m# We have our features\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m features\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/femr/src/femr/models/transformer.py:394\u001b[0m, in \u001b[0;36mcompute_features\u001b[0;34m(dataset, model_path, labels, num_proc, tokens_per_batch, device, ontology)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m device:\n\u001b[1;32m    392\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 394\u001b[0m batches \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39;49mconvert_dataset(\n\u001b[1;32m    395\u001b[0m     filtered_data, tokens_per_batch\u001b[39m=\u001b[39;49mtokens_per_batch, min_samples_per_batch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, num_proc\u001b[39m=\u001b[39;49mnum_proc\n\u001b[1;32m    396\u001b[0m )\n\u001b[1;32m    398\u001b[0m batches\u001b[39m.\u001b[39mset_format(\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    400\u001b[0m all_patient_ids \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/femr/src/femr/models/processor.py:314\u001b[0m, in \u001b[0;36mFEMRBatchProcessor.convert_dataset\u001b[0;34m(self, dataset, tokens_per_batch, min_samples_per_batch, num_proc)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCreating batches\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(batches))\n\u001b[1;32m    308\u001b[0m batch_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[1;32m    309\u001b[0m     _batch_generator,\n\u001b[1;32m    310\u001b[0m     creator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreator,\n\u001b[1;32m    311\u001b[0m     dataset\u001b[39m=\u001b[39mdataset,\n\u001b[1;32m    312\u001b[0m )\n\u001b[0;32m--> 314\u001b[0m batch_dataset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mDataset\u001b[39m.\u001b[39;49mfrom_generator(\n\u001b[1;32m    315\u001b[0m     batch_func,\n\u001b[1;32m    316\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m    317\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbatch_data\u001b[39;49m\u001b[39m\"\u001b[39;49m: final_batch_data,\n\u001b[1;32m    318\u001b[0m     },\n\u001b[1;32m    319\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    320\u001b[0m     writer_batch_size\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m    321\u001b[0m )\n\u001b[1;32m    323\u001b[0m \u001b[39mreturn\u001b[39;00m batch_dataset\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/arrow_dataset.py:1072\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a Dataset from a generator.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \n\u001b[1;32m   1017\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[39m```\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerator\u001b[39;00m \u001b[39mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[1;32m   1064\u001b[0m \u001b[39mreturn\u001b[39;00m GeneratorDatasetInputStream(\n\u001b[1;32m   1065\u001b[0m     generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m   1066\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1067\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1068\u001b[0m     keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m   1069\u001b[0m     gen_kwargs\u001b[39m=\u001b[39;49mgen_kwargs,\n\u001b[1;32m   1070\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1071\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m-> 1072\u001b[0m )\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/io/generator.py:47\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     verification_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     base_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m     48\u001b[0m         download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m     49\u001b[0m         download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m     50\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m     51\u001b[0m         try_from_hf_gcs\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     52\u001b[0m         base_path\u001b[39m=\u001b[39;49mbase_path,\n\u001b[1;32m     53\u001b[0m         num_proc\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m     54\u001b[0m     )\n\u001b[1;32m     55\u001b[0m     dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mas_dataset(\n\u001b[1;32m     56\u001b[0m         split\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m, verification_mode\u001b[39m=\u001b[39mverification_mode, in_memory\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_in_memory\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/builder.py:948\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 948\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    949\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    950\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    951\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    952\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    953\u001b[0m     )\n\u001b[1;32m    954\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/builder.py:1711\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1711\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1712\u001b[0m         dl_manager,\n\u001b[1;32m   1713\u001b[0m         verification_mode,\n\u001b[1;32m   1714\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1715\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1716\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1717\u001b[0m     )\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/builder.py:1043\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[1;32m   1041\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[1;32m   1044\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1045\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1046\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1048\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m   1050\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/builder.py:1578\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[39mwith\u001b[39;00m Pool(num_proc) \u001b[39mas\u001b[39;00m pool:\n\u001b[1;32m   1577\u001b[0m     \u001b[39mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1578\u001b[0m         \u001b[39mfor\u001b[39;00m job_id, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   1579\u001b[0m             pool, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_split_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[1;32m   1580\u001b[0m         ):\n\u001b[1;32m   1581\u001b[0m             \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   1582\u001b[0m                 \u001b[39m# the content is the result of the job\u001b[39;00m\n\u001b[1;32m   1583\u001b[0m                 (\n\u001b[1;32m   1584\u001b[0m                     examples_per_job[job_id],\n\u001b[1;32m   1585\u001b[0m                     bytes_per_job[job_id],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1588\u001b[0m                     shard_lengths_per_job[job_id],\n\u001b[1;32m   1589\u001b[0m                 ) \u001b[39m=\u001b[39m content\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[1;32m   1416\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m         [async_result\u001b[39m.\u001b[39mget(timeout\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/datasets/utils/py_utils.py:1417\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[1;32m   1416\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m-> 1417\u001b[0m         [async_result\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m) \u001b[39mfor\u001b[39;00m async_result \u001b[39min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/multiprocess/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "import femr.models.transformer\n",
    "import pyarrow.csv\n",
    "import datasets\n",
    "import pickle\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "# First, we compute our features\n",
    "\n",
    "# Load some labels\n",
    "# labels = pyarrow.csv.read_csv('input/labels.csv').to_pylist()\n",
    "# label_csv = '/share/pi/nigam/projects/zphuo/data/PE/inspect/cohort_0.2.0_master_file_anon.csv'\n",
    "label_csv_subset = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/cohort_0.2.0_master_file_anon_subset.csv'\n",
    "\n",
    "labels_table = pyarrow.csv.read_csv(label_csv_subset)\n",
    "\n",
    "# filter out censored\n",
    "selected_table = labels_table.select(['patient_id', 'procedure_time', '12_month_PH'])\n",
    "filtered_table = selected_table.filter(pa.compute.field(\"12_month_PH\") != \"Censored\")\n",
    "\n",
    "# cast to bool\n",
    "casted_column = pc.cast(filtered_table.column('12_month_PH'), target_type=pa.bool_())\n",
    "filtered_table = filtered_table.set_column(filtered_table.schema.get_field_index('12_month_PH'), pa.field('12_month_PH', pa.bool_()), casted_column)\n",
    "\n",
    "\n",
    "columns = {name: filtered_table.column(name) for name in filtered_table.column_names}\n",
    "columns['prediction_time'] = columns.pop('procedure_time')\n",
    "columns['boolean_value'] = columns.pop('12_month_PH')\n",
    "filtered_table = pa.Table.from_arrays(list(columns.values()), names=list(columns.keys()))\n",
    "\n",
    "labels = filtered_table.to_pylist()\n",
    "\n",
    "# Load our data\n",
    "# dataset = datasets.Dataset.from_parquet(\"input/meds/data/*\")\n",
    "parquet_folder = '/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds/data_subset/*'\n",
    "dataset = datasets.Dataset.from_parquet(parquet_folder)\n",
    "\n",
    "# femr.ontology.Ontology will create one given an athena path and code_metadata\n",
    "# We need an ontology for MOTOR\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "model_folder = '/share/pi/nigam/projects/zphuo/repos/femr/tutorials/trash/tutorial_6_INSEPCT/motor_model'\n",
    "# model_folder = '/share/pi/nigam/ethanid/femrv3/femr/tutorials/trash/tutorial_6/motor_model'\n",
    "\n",
    "\n",
    "features = femr.models.transformer.compute_features(dataset, model_folder, labels, num_proc=num_proc, tokens_per_batch=128, ontology=ontology)\n",
    "\n",
    "# We have our features\n",
    "for k, v in features.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c75a9",
   "metadata": {},
   "source": [
    "# Joining features and labels\n",
    "\n",
    "Given a feature set, it's important to be able to join a set of labels to those features.\n",
    "\n",
    "This can be done with femr.featurizers.join_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad882f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolean_values (200,)\n",
      "patient_ids (200,)\n",
      "times (200,)\n",
      "features (200, 64)\n"
     ]
    }
   ],
   "source": [
    "import femr.featurizers\n",
    "\n",
    "features_and_labels = femr.featurizers.join_labels(features, labels)\n",
    "\n",
    "for k, v in features_and_labels.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192ccc8",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "When using a pretrained CLMBR model, we have to be very careful to use the splits used for the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c49417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import femr.splits\n",
    "import numpy as np\n",
    "\n",
    "# We split into a global training and test set\n",
    "split = femr.splits.PatientSplit.load_from_csv('input/motor_model/main_split.csv')\n",
    "\n",
    "train_mask = np.isin(features_and_labels['patient_ids'], split.train_patient_ids)\n",
    "test_mask = np.isin(features_and_labels['patient_ids'], split.test_patient_ids)\n",
    "\n",
    "percent_train = .70\n",
    "X_train, y_train = (\n",
    "    features_and_labels['features'][train_mask],\n",
    "    features_and_labels['boolean_values'][train_mask],\n",
    ")\n",
    "X_test, y_test = (\n",
    "    features_and_labels['features'][test_mask],\n",
    "    features_and_labels['boolean_values'][test_mask],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deca785",
   "metadata": {},
   "source": [
    "# Building Models\n",
    "\n",
    "The generated features can then be used to build your standard models. In this case we construct both logistic regression and XGBoost models and evaluate them.\n",
    "\n",
    "Performance is perfect since our task (predicting gender) is 100% determined by the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5ad4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Logistic Regression ----\n",
      "Train:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n",
      "Test:\n",
      "\tAUROC: 1.0\n",
      "\tAPS: 1.0\n",
      "\tAccuracy: 1.0\n",
      "\tF1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.preprocessing\n",
    "\n",
    "def run_analysis(title: str, y_train, y_train_proba, y_test, y_test_proba):\n",
    "    print(f\"---- {title} ----\")\n",
    "    print(\"Train:\")\n",
    "    print_metrics(y_train, y_train_proba)\n",
    "    print(\"Test:\")\n",
    "    print_metrics(y_test, y_test_proba)\n",
    "\n",
    "def print_metrics(y_true, y_proba):\n",
    "    y_pred = y_proba > 0.5\n",
    "    auroc = sklearn.metrics.roc_auc_score(y_true, y_proba)\n",
    "    aps = sklearn.metrics.average_precision_score(y_true, y_proba)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred)\n",
    "    print(\"\\tAUROC:\", auroc)\n",
    "    print(\"\\tAPS:\", aps)\n",
    "    print(\"\\tAccuracy:\", accuracy)\n",
    "    print(\"\\tF1 Score:\", f1)\n",
    "\n",
    "\n",
    "model = sklearn.linear_model.LogisticRegressionCV(penalty=\"l2\", solver=\"liblinear\").fit(X_train, y_train)\n",
    "y_train_proba = model.predict_proba(X_train)[::, 1]\n",
    "y_test_proba = model.predict_proba(X_test)[::, 1]\n",
    "run_analysis(\"Logistic Regression\", y_train, y_train_proba, y_test, y_test_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
