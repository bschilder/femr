{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train MOTOR\n",
    "\n",
    "This tutorial walks through the various steps to train a MOTOR model.\n",
    "\n",
    "Training MOTOR is a four step process:\n",
    "\n",
    "- Training a tokenizer\n",
    "- Prefitting MOTOR\n",
    "- Preparing batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_6'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646f7590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c216c12d394ca284554b83f80eb616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 4, 6, 7, 10, 11, 12, 13, 14, 15, 18, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 33, 36, 37, 38, 40, 42, 44, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 69, 70, 73, 74, 75, 76, 77, 79, 80, 83, 85, 86, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 107, 109, 110, 112, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 133, 134, 135, 136, 137, 139, 141, 142, 143, 144, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 168, 169, 171, 172, 173, 174, 178, 181, 182, 183, 184, 185, 186, 187, 189, 192, 193, 195, 196, 197, 198, 199]\n",
      "[19, 22, 25, 39, 46, 71, 82, 84, 87, 92, 106, 108, 113, 131, 132, 138, 146, 147, 148, 155, 177, 179, 180, 188, 190, 191]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e67883171e4436081905134b9ba9e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'events'],\n",
      "        num_rows: 144\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'events'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import femr.index\n",
    "import femr.splits\n",
    "\n",
    "# First, we want to split our dataset into train, valid, and test\n",
    "# We do this by calling our split functionality twice\n",
    "\n",
    "dataset = datasets.Dataset.from_parquet('input/meds/data/*')\n",
    "\n",
    "\n",
    "index = femr.index.PatientIndex(dataset, num_proc=4)\n",
    "main_split = femr.splits.generate_hash_split(index.get_patient_ids(), 97, frac_test=0.15)\n",
    "\n",
    "os.mkdir(os.path.join(TARGET_DIR, 'motor_model'))\n",
    "# Note that we want to save this to the target directory since this is important information\n",
    "\n",
    "main_split.save_to_csv(os.path.join(TARGET_DIR, \"motor_model\", \"main_split.csv\"))\n",
    "\n",
    "train_split = femr.splits.generate_hash_split(main_split.train_patient_ids, 87, frac_test=0.15)\n",
    "\n",
    "print(train_split.train_patient_ids)\n",
    "print(train_split.test_patient_ids)\n",
    "\n",
    "main_dataset = main_split.split_dataset(dataset, index)\n",
    "train_dataset = train_split.split_dataset(main_dataset['train'], femr.index.PatientIndex(main_dataset['train'], num_proc=4))\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e92ab8218c4f3f8823ed136f0fef78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import femr.models.tokenizer\n",
    "import pickle\n",
    "\n",
    "# First, we need to train a tokenizer\n",
    "# Note, we need to use a hierarchical tokenizer for MOTOR\n",
    "\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "tokenizer = femr.models.tokenizer.train_tokenizer(\n",
    "    main_dataset['train'], vocab_size=128, is_hierarchical=True, num_proc=4, ontology=ontology)\n",
    "\n",
    "# Save the tokenizer to the same directory as the model\n",
    "tokenizer.save_pretrained(os.path.join(TARGET_DIR, \"motor_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b60daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726caa5a60484f9c89a99018e9053e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import femr.models.tasks\n",
    "\n",
    "# Second, we need to prefit the MOTOR model. This is necessary because piecewise exponential models are unstable without an initial fit\n",
    "\n",
    "motor_task = femr.models.tasks.MOTORTask.fit_pretraining_task_info(\n",
    "    main_dataset['train'], tokenizer, num_tasks=64, num_bins=4, final_layer_size=32, num_proc=4)\n",
    "\n",
    "\n",
    "# It's recommended to save this with pickle to avoid recomputing since it's an expensive operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1be962db9af4b67b72c94adde189217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11aac5563f2346c59830968fc41731de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b82e14cbfbd4202aed8422be7bbef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 4 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37547e0dbbc648f4a81d04230b4db9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import femr.models.processor\n",
    "import femr.models.tasks\n",
    "\n",
    "# Third, we need to create batches. \n",
    "\n",
    "processor = femr.models.processor.FEMRBatchProcessor(tokenizer, motor_task)\n",
    "\n",
    "# We can do this one patient at a time\n",
    "example_batch = processor.convert_patient(train_dataset['train'][0])\n",
    "\n",
    "# But generally we want to convert entire datasets\n",
    "train_batches = processor.convert_dataset(train_dataset, tokens_per_batch=32, num_proc=4)\n",
    "\n",
    "# Convert our batches to pytorch tensors\n",
    "train_batches.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 00:42, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.463600</td>\n",
       "      <td>0.457390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.457474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.446200</td>\n",
       "      <td>0.457564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.457646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.457723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.457794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.458900</td>\n",
       "      <td>0.457868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.457944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.458007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.458067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.474700</td>\n",
       "      <td>0.458128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.458183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.454800</td>\n",
       "      <td>0.458237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.458290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.458330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.458374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.439200</td>\n",
       "      <td>0.458415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.438800</td>\n",
       "      <td>0.458447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.458482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.423300</td>\n",
       "      <td>0.458515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.458542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.458566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.406500</td>\n",
       "      <td>0.458589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.454400</td>\n",
       "      <td>0.458606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.458622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.422800</td>\n",
       "      <td>0.458634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.455300</td>\n",
       "      <td>0.458645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>0.458652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.437100</td>\n",
       "      <td>0.458657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.458658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory tmp_trainer/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# Finally, given the batches, we can train CLMBR.\n",
    "# We can use huggingface's trainer to do this.\n",
    "\n",
    "transformer_config = femr.models.transformer.FEMRTransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    is_hierarchical=tokenizer.is_hierarchical, \n",
    "    n_layers=2,\n",
    "    hidden_size=64, \n",
    "    intermediate_size=64*2,\n",
    "    n_heads=8,\n",
    ")\n",
    "\n",
    "config = femr.models.transformer.FEMRModelConfig.from_transformer_task_configs(transformer_config, motor_task.get_task_config())\n",
    "\n",
    "model = femr.models.transformer.FEMRModel(config)\n",
    "\n",
    "\n",
    "def collate_fn(a):\n",
    "    assert len(a) == 1\n",
    "    return {'batch': a[0]}\n",
    "\n",
    "trainer_config = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    output_dir='tmp_trainer',\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "\n",
    "    eval_steps=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    logging_steps=20,\n",
    "    logging_strategy='steps',\n",
    "\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(*args, **kwargs):\n",
    "    print(\"Cmopute metrics\")\n",
    "    print(args, kwargs)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_batches['train'],\n",
    "    eval_dataset=train_batches['test'],\n",
    "    compute_metrics=None,\n",
    "    args=trainer_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(os.path.join(TARGET_DIR, 'motor_model'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
