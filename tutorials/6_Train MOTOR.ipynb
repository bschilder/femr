{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c81279e-a568-4e36-9906-06317accb622",
   "metadata": {},
   "source": [
    "# Train MOTOR\n",
    "\n",
    "This tutorial walks through the various steps to train a MOTOR model.\n",
    "\n",
    "Training MOTOR is a four step process:\n",
    "\n",
    "- Training a tokenizer\n",
    "- Prefitting MOTOR\n",
    "- Preparing batches\n",
    "- Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dcdfd70-58a1-4460-80a8-db737a8c5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = '/share/pi/nigam/zphuo/cache_dir'\n",
    "\n",
    "\n",
    "TARGET_DIR = 'trash/tutorial_6'\n",
    "\n",
    "if os.path.exists(TARGET_DIR):\n",
    "    shutil.rmtree(TARGET_DIR)\n",
    "\n",
    "os.mkdir(TARGET_DIR)\n",
    "\n",
    "num_proc = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646f7590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72238421002e4cceaa4443f779e72dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 4, 6, 7, 10, 11, 12, 13, 14, 15, 18, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 33, 36, 37, 38, 40, 42, 44, 45, 47, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 61, 62, 63, 64, 65, 66, 67, 69, 70, 73, 74, 75, 76, 77, 79, 80, 83, 85, 86, 88, 89, 90, 91, 93, 94, 95, 96, 97, 98, 100, 101, 102, 103, 104, 105, 107, 109, 110, 112, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 133, 134, 135, 136, 137, 139, 141, 142, 143, 144, 149, 150, 151, 152, 153, 154, 156, 157, 158, 159, 160, 161, 162, 163, 165, 166, 168, 169, 171, 172, 173, 174, 178, 181, 182, 183, 184, 185, 186, 187, 189, 192, 193, 195, 196, 197, 198, 199]\n",
      "[19, 22, 25, 39, 46, 71, 82, 84, 87, 92, 106, 108, 113, 131, 132, 138, 146, 147, 148, 155, 177, 179, 180, 188, 190, 191]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c552a8da2294526b6151ad11af93cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['patient_id', 'events'],\n",
      "        num_rows: 144\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['patient_id', 'events'],\n",
      "        num_rows: 26\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import femr.index\n",
    "import femr.splits\n",
    "\n",
    "# First, we want to split our dataset into train, valid, and test\n",
    "# We do this by calling our split functionality twice\n",
    "\n",
    "dataset = datasets.Dataset.from_parquet('input/meds/data/*')\n",
    "# dataset = datasets.Dataset.from_parquet('/share/pi/nigam/projects/zphuo/data/PE/inspect/timelines_smallfiles_meds_3/data/*')\n",
    "\n",
    "\n",
    "index = femr.index.PatientIndex(dataset, num_proc=num_proc)\n",
    "main_split = femr.splits.generate_hash_split(index.get_patient_ids(), 97, frac_test=0.15)\n",
    "\n",
    "os.mkdir(os.path.join(TARGET_DIR, 'motor_model'))\n",
    "# Note that we want to save this to the target directory since this is important information\n",
    "\n",
    "main_split.save_to_csv(os.path.join(TARGET_DIR, \"motor_model\", \"main_split.csv\"))\n",
    "\n",
    "train_split = femr.splits.generate_hash_split(main_split.train_patient_ids, 87, frac_test=0.15)\n",
    "\n",
    "print(train_split.train_patient_ids)\n",
    "print(train_split.test_patient_ids)\n",
    "\n",
    "main_dataset = main_split.split_dataset(dataset, index)\n",
    "train_dataset = train_split.split_dataset(main_dataset['train'], femr.index.PatientIndex(main_dataset['train'], num_proc=num_proc))\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89348e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['patient_id', 'events'],\n",
       "    num_rows: 170\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8360f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60ab7df-e851-44a5-ab70-7bee292be00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fc23ba59a34887a8fd14e270e9bc1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import femr.models.tokenizer\n",
    "import pickle\n",
    "\n",
    "# First, we need to train a tokenizer\n",
    "# Note, we need to use a hierarchical tokenizer for MOTOR\n",
    "\n",
    "with open('input/meds/ontology.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "tokenizer = femr.models.tokenizer.train_tokenizer(\n",
    "    main_dataset['train'], vocab_size=128, is_hierarchical=True, num_proc=num_proc, ontology=ontology)\n",
    "\n",
    "# Save the tokenizer to the same directory as the model\n",
    "tokenizer.save_pretrained(os.path.join(TARGET_DIR, \"motor_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b60daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11c5d6962764c5abad98326ae7c5443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/170 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import femr.models.tasks\n",
    "\n",
    "# Second, we need to prefit the MOTOR model. This is necessary because piecewise exponential models are unstable without an initial fit\n",
    "\n",
    "motor_task = femr.models.tasks.MOTORTask.fit_pretraining_task_info(\n",
    "    main_dataset['train'], tokenizer, num_tasks=64, num_bins=4, final_layer_size=32, num_proc=num_proc)\n",
    "\n",
    "\n",
    "# It's recommended to save this with pickle to avoid recomputing since it's an expensive operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89611ba9-a242-4b87-9b8f-25670d838fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert a single patient\n",
      "Convert batches\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dfc07d04dc407a9b6b552955856534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/144 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed29bc54081465f9b9477b0e6c6e91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b5d2d1f316488f9b0525210b92d8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/26 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 8 to 2 for the train split as it only contains 2 shards.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3c28f10be64305817094fae55f63f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert batches to pytorch\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import femr.models.processor\n",
    "import femr.models.tasks\n",
    "\n",
    "# Third, we need to create batches. \n",
    "\n",
    "processor = femr.models.processor.FEMRBatchProcessor(tokenizer, motor_task)\n",
    "\n",
    "# We can do this one patient at a time\n",
    "print(\"Convert a single patient\")\n",
    "example_batch = processor.collate([processor.convert_patient(train_dataset['train'][0], tensor_type='pt')])\n",
    "\n",
    "print(\"Convert batches\")\n",
    "# But generally we want to convert entire datasets\n",
    "train_batches = processor.convert_dataset(train_dataset, tokens_per_batch=36, num_proc=num_proc)\n",
    "\n",
    "print(\"Convert batches to pytorch\")\n",
    "# Convert our batches to pytorch tensors\n",
    "train_batches.set_format(\"pt\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f654a46c-5aa7-465c-b6c5-73d8ba26ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-08 04:22:22.889080: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-08 04:22:29.158993: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/pi/nigam/projects/zphuo/cuda/lib64::/home/zphuo/packages/cuda/lib64:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/bin\n",
      "2024-02-08 04:22:29.159200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /share/pi/nigam/projects/zphuo/cuda/lib64::/home/zphuo/packages/cuda/lib64:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/lib:/home/zphuo/miniconda3/bin\n",
      "2024-02-08 04:22:29.159215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# Finally, given the batches, we can train CLMBR.\n",
    "# We can use huggingface's trainer to do this.\n",
    "\n",
    "transformer_config = femr.models.transformer.FEMRTransformerConfig(\n",
    "    vocab_size=tokenizer.vocab_size, \n",
    "    is_hierarchical=tokenizer.is_hierarchical, \n",
    "    n_layers=2,\n",
    "    hidden_size=64, \n",
    "    intermediate_size=64*2,\n",
    "    n_heads=8,\n",
    ")\n",
    "\n",
    "config = femr.models.transformer.FEMRModelConfig.from_transformer_task_configs(transformer_config, motor_task.get_task_config())\n",
    "\n",
    "model = femr.models.transformer.FEMRModel(config)\n",
    "\n",
    "collator = processor.collate\n",
    "\n",
    "trainer_config = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "\n",
    "    output_dir='tmp_trainer',\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=100,\n",
    "\n",
    "    eval_steps=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    logging_steps=20,\n",
    "    logging_strategy='steps',\n",
    "\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    data_collator=processor.collate,\n",
    "    train_dataset=train_batches['train'],\n",
    "    eval_dataset=train_batches['test'],\n",
    "    args=trainer_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c67b7993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ages': tensor([  0.,   0.,   0.,   0.,   0.,   0., 270., 436.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,  21.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           82., 188., 273.,   0.,   0.,   0.,  42.,   0.,   0.,   0.]),\n",
       "  'hierarchical_tokens': tensor([  0,   1,   3,   0,   1,   4, 118,  42,  20,  11,  95,   5,  40,  82,\n",
       "           28,   6,   0,   1,   4,   0,   1,   4,   0,   1,   4,  17,  12,   5,\n",
       "           54,  41,  52,  89,  11,   8,  42,  20,  23,  95,  47,  40,  14, 103,\n",
       "           45,  28, 108,   6, 113,  62, 101,  51,   7,  57,  93,  90,  48,  18,\n",
       "           82,  91,  79,  58,  27,  84,   0,   1,   4,   0,   1,   4,   7,  19,\n",
       "           44,  74,  16,  50,   9,   0,   2,   3, 121,   0,   1,   3]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1111, 0.1111,\n",
       "          0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0270, 0.0270,\n",
       "          0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
       "          0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
       "          0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
       "          0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.3333, 0.3333,\n",
       "          0.3333, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000]),\n",
       "  'label_indices': tensor([ 2,  5,  6,  7, 10, 13, 16, 17, 20, 23, 24, 25, 26, 29, 30, 33]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -0.1833,  0.3928,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.0475, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -0.8358, -0.4679, -0.1729, -1.1204, -1.1204, -1.1204, -0.9746, -1.1204,\n",
       "          -1.1204, -1.1204]),\n",
       "  'patient_lengths': tensor([3, 5, 3, 3, 4, 3, 6, 4, 3]),\n",
       "  'timestamps': tensor([707011200, 707011200, 707011200, 711072000, 711072000, 711072000,\n",
       "          734400000, 748742400, 653616000, 653616000, 653616000, 701308800,\n",
       "          701308800, 701308800, 709603200, 709603200, 709603200, 711417600,\n",
       "          644716800, 644716800, 644716800, 650073600, 650073600, 650073600,\n",
       "          657158400, 666316800, 673660800, 662601600, 662601600, 662601600,\n",
       "          666230400, 667094400, 667094400, 667094400]),\n",
       "  'token_indices': tensor([ 0,  1,  2,  3,  4,  5,  6,  7, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
       "          62, 63, 64, 65, 66, 67, 68, 70, 73, 75, 76, 77, 78, 79, 80, 81, 82]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([  0.,   0.,   0.,  52., 521.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,  67.,  86.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0., 257., 494., 685., 970.,   0.,   0.,   0.,   0.,   0.,   0.,  66.]),\n",
       "  'hierarchical_tokens': tensor([  0,   1,   4,  33,  17,  78,  12,   5,  65,  41,  49, 116,   8,  20,\n",
       "           10,  23,  31,   9, 103,  85,  45,  34,  81,  28, 108,   6,  39,  46,\n",
       "           99,  98,   7,  36, 115,  30,  57,  35,  93,  90,  53,  48,  18,  80,\n",
       "           79, 105, 100,  19,  13,  61,  27,  84,  21,  44,  32,  16,  74,   0,\n",
       "            1,   4,   0,   2,   3,   0,   2,   3,   0,   2,   4,  56,  55,   8,\n",
       "           42,  20,  11,  62,  95,  12,   5,  88,  41,  82,  97, 103,  28,   6,\n",
       "          108,  84,   0,   2,   4,   0,   1,   3,  12,   5,  41,  29,  52,  89,\n",
       "           73, 102,  20,  24,  22,  94,  87,  40,  70,  28,  26, 108,   6, 109,\n",
       "          101,  77,  51,   7,  90,  91,  79, 105,  19,  13,  61,  27,  84,  55,\n",
       "            8,  22,   6,  15,   0,   1,   4,   0,   1,   3,  33,   9,  34,  35,\n",
       "           49]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "          0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "          0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "          0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "          0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208,\n",
       "          0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.0208, 0.2500, 0.2500, 0.2500,\n",
       "          0.2500, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 0.3333, 0.3333, 0.3333, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "          0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "          0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "          0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.5000,\n",
       "          0.5000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000]),\n",
       "  'label_indices': tensor([ 2,  3,  4,  7, 10, 13, 16, 17, 18, 21, 24, 25, 26, 27, 28, 31, 34, 35]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -0.9399,  0.6878, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -0.8878, -0.8219, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -0.2284,  0.5941,  1.2569,  2.2460, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -0.8913]),\n",
       "  'patient_lengths': tensor([5, 3, 3, 3, 5, 3, 7, 3, 4]),\n",
       "  'timestamps': tensor([683769600, 683769600, 683769600, 688262400, 728784000, 654393600,\n",
       "          654393600, 654393600, 698284800, 698284800, 698284800, 704937600,\n",
       "          704937600, 704937600, 703209600, 703209600, 703209600, 708998400,\n",
       "          710640000, 667180800, 667180800, 667180800, 704246400, 704246400,\n",
       "          704246400, 726451200, 746928000, 763430400, 788054400, 652924800,\n",
       "          652924800, 652924800, 659318400, 659318400, 659318400, 665020800]),\n",
       "  'token_indices': tensor([  0,   1,   2,   3,  51,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
       "           64,  65,  66,  67,  70,  86,  87,  88,  89,  90,  91,  92, 125, 127,\n",
       "          129, 130, 131, 132, 133, 134, 135, 136, 141]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([  0.,   0.,   0.,   0.,   0.,   0., 176.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0., 494.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0., 475.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'hierarchical_tokens': tensor([  0,   1,   3,   0,   2,   3, 117,   0,   2,   3,   0,   1,   3,   0,\n",
       "            1,   3,  22, 110,   6,   0,   2,   3,   0,   1,   3,   0,   2,   4,\n",
       "            0,   1,   3,  67,  42,  10,  33,  37,  15, 104,  38,  66,  92,   9,\n",
       "           43, 111,  11,   0,   2,   4,   0,   1,   4]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.3333, 0.3333,\n",
       "          0.3333, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "          0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714, 0.0714,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "  'label_indices': tensor([ 2,  5,  6,  9, 12, 15, 16, 19, 22, 25, 28, 29, 32, 35]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -0.5096, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "           0.5941, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,  0.5281, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204]),\n",
       "  'patient_lengths': tensor([3, 4, 3, 3, 4, 3, 3, 3, 4, 3, 3]),\n",
       "  'timestamps': tensor([646531200, 646531200, 646531200, 657763200, 657763200, 657763200,\n",
       "          672969600, 648432000, 648432000, 648432000, 693532800, 693532800,\n",
       "          693532800, 679190400, 679190400, 679190400, 721872000, 695433600,\n",
       "          695433600, 695433600, 688348800, 688348800, 688348800, 649900800,\n",
       "          649900800, 649900800, 707097600, 707097600, 707097600, 748137600,\n",
       "          681177600, 681177600, 681177600, 709257600, 709257600, 709257600]),\n",
       "  'token_indices': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 19,\n",
       "          20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 45, 46, 47, 48, 49, 50,\n",
       "          51]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 732., 865.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0., 886., 949.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'hierarchical_tokens': tensor([  0,   1,   4,   0,   2,   4,   0,   1,   3,  59,  60,  10,  23,  24,\n",
       "           48,   7,  80,  36,   9,  43,  49,   8, 120,   0,   2,   3,   0,   1,\n",
       "            3,   0,   1,   4,   0,   1,   3,   0,   1,   3,  17,  12,   5,  66,\n",
       "           43, 116,   8,  59,  10,  23,  31,  87,   9,  14,  85,   6,  60, 113,\n",
       "           62,  99,  98,   7, 115,  57,  93,  48,  18,  79,  21,  22,   6,   0,\n",
       "            1,   4,   0,   2,   4]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
       "          0.0769, 0.0769, 0.0769, 0.0769, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345,\n",
       "          0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345,\n",
       "          0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345, 0.0345,\n",
       "          0.0345, 0.0345, 0.0345, 0.0345, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000]),\n",
       "  'label_indices': tensor([ 2,  5,  8,  9, 10, 13, 16, 19, 22, 25, 26, 27, 30, 33]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204,  1.4201,  1.8816, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204,  1.9545,  2.1732, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204]),\n",
       "  'patient_lengths': tensor([3, 3, 5, 3, 3, 3, 3, 5, 3, 3]),\n",
       "  'timestamps': tensor([690336000, 690336000, 690336000, 706320000, 706320000, 706320000,\n",
       "          717033600, 717033600, 717033600, 780278400, 791769600, 676944000,\n",
       "          676944000, 676944000, 652233600, 652233600, 652233600, 658022400,\n",
       "          658022400, 658022400, 715737600, 715737600, 715737600, 684720000,\n",
       "          684720000, 684720000, 761270400, 766713600, 646444800, 646444800,\n",
       "          646444800, 702777600, 702777600, 702777600]),\n",
       "  'token_indices': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 22, 23, 24, 25, 26, 27, 28, 29,\n",
       "          30, 31, 32, 33, 34, 35, 36, 37, 38, 67, 69, 70, 71, 72, 73, 74, 75]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([  0.,   0.,   0.,  73., 723., 770.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 174.,   0.,   0.,\n",
       "            0., 127., 156., 372., 635.,   0.,   0.,   0., 123.]),\n",
       "  'hierarchical_tokens': tensor([  0,   1,   4,   6, 123,   5,   0,   2,   3,   0,   2,   4,   0,   2,\n",
       "            3,   0,   2,   3,   0,   2,   3,  12,   5,  54,  29, 116,   8,  23,\n",
       "           24,  47,  87,  70,   9,  34,  28,  26, 108,   6,  77,   7,  86,  35,\n",
       "           90,  97,  79, 105,  19,  58,  13,  27,  84,  21,   0,   1,   4,  17,\n",
       "           65,   5,  49, 116,   8,  20,  10,  23,   9,  14,  39, 113,  98,   7,\n",
       "           36,  86, 115,  30,  57,  83,  53,  48,  18,  58,  21,  56,  55,   8,\n",
       "           12,   5,  41,  29,  52,  89, 102,  20,  24,  22,  94,  87,  40,  70,\n",
       "           45,  28,  26, 108,   6, 109, 101,  77,  51,   7,  90,  91,  79, 105,\n",
       "           19,  13,  61,  27,  84,  42,  20,  11,  95,   5,  41,  82,  28,   6,\n",
       "            0,   1,   4, 127]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
       "          0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
       "          0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323,\n",
       "          0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 0.0323, 1.0000, 1.0000,\n",
       "          1.0000, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
       "          0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
       "          0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385, 0.0385,\n",
       "          0.3333, 0.3333, 0.3333, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "          0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "          0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "          0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303, 0.0303,\n",
       "          0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "  'label_indices': tensor([ 2,  3,  4,  5,  8, 11, 14, 17, 20, 21, 24, 25, 26, 27, 28, 31, 32]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -0.8670,  1.3888,  1.5519, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -0.5165, -1.1204, -1.1204,\n",
       "          -1.1204, -0.6796, -0.5790,  0.1707,  1.0834, -1.1204, -1.1204, -1.1204,\n",
       "          -0.6935]),\n",
       "  'patient_lengths': tensor([6, 3, 3, 3, 3, 4, 7, 4]),\n",
       "  'timestamps': tensor([683078400, 683078400, 683078400, 689385600, 745545600, 749606400,\n",
       "          658972800, 658972800, 658972800, 669427200, 669427200, 669427200,\n",
       "          681091200, 681091200, 681091200, 696643200, 696643200, 696643200,\n",
       "          644112000, 644112000, 644112000, 659145600, 684028800, 684028800,\n",
       "          684028800, 695001600, 697507200, 716169600, 738892800, 686707200,\n",
       "          686707200, 686707200, 697334400]),\n",
       "  'token_indices': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "           14,  15,  16,  17,  18,  19,  20,  21,  52,  53,  54,  55,  81,  84,\n",
       "          117, 126, 127, 128, 129, 130]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([  0.,   0.,   0.,  97.,   0.,   0.,   0.,  18.,  86.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0., 253., 326.,   0.,   0.,   0.,  62.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 459., 662.]),\n",
       "  'hierarchical_tokens': tensor([  0,   2,   4,  81,  13,   0,   1,   3,  68,  17,   5,  96,  24,  26,\n",
       "           98,   7,   5,   0,   1,   4,   0,   1,   3,  14, 112,  60,  10,  55,\n",
       "           31,  38,  69,  43,  11,   8,   6,   0,   2,   3,  39,  17,  53,   5,\n",
       "           18,   7, 100,   0,   2,   3,   0,   1,   3,   0,   2,   4,   0,   1,\n",
       "            4,  67,  42,  37,  38,  92,   9,  82,  34,  75, 111,  73,  11,  77,\n",
       "           13]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 0.1250,\n",
       "          0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.1000, 0.1000,\n",
       "          0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 1.0000,\n",
       "          1.0000, 1.0000, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833,\n",
       "          0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.0833, 0.5000, 0.5000]),\n",
       "  'label_indices': tensor([ 2,  3,  6,  7,  8, 11, 14, 15, 16, 19, 20, 23, 26, 29, 32, 33, 34]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -0.7837, -1.1204, -1.1204, -1.1204, -1.0579,\n",
       "          -0.8219, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -0.2423,\n",
       "           0.0110, -1.1204, -1.1204, -1.1204, -0.9052, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204,  0.4726,  1.1771]),\n",
       "  'patient_lengths': tensor([4, 5, 3, 5, 4, 3, 3, 3, 5]),\n",
       "  'timestamps': tensor([694915200, 694915200, 694915200, 703296000, 714268800, 714268800,\n",
       "          714268800, 715824000, 721699200, 689731200, 689731200, 689731200,\n",
       "          674179200, 674179200, 674179200, 696038400, 702345600, 678499200,\n",
       "          678499200, 678499200, 683856000, 658195200, 658195200, 658195200,\n",
       "          641952000, 641952000, 641952000, 663292800, 663292800, 663292800,\n",
       "          650246400, 650246400, 650246400, 689904000, 707443200]),\n",
       "  'token_indices': tensor([ 0,  1,  2,  3,  5,  6,  7,  8, 16, 17, 18, 19, 20, 21, 22, 23, 25, 35,\n",
       "          36, 37, 38, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 69, 71]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "           78., 222.,   0.,   0.,   0.,   0.,   0.,   0., 192., 352.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]),\n",
       "  'hierarchical_tokens': tensor([  0,   2,   3,   0,   2,   3,   0,   1,   3,   0,   2,   3,  17,  78,\n",
       "           12,   5,  54,  43,  52,  49, 116,   8,  59,  10,  25,  23,  24,  31,\n",
       "           40,   9,  14,  45,  34,  28, 108,   6,  39,  46,  60,  55,  99,  98,\n",
       "            7,  36, 115,  30,  57,  64,  35,  93,  53,  48,  18,  88, 114,  80,\n",
       "           79, 100,  75,  15,  27,  84,  21,  67,  42,  10,  37,  66,  92,   9,\n",
       "           43, 111,  11,   0,   2,   4,   0,   1,   3,  33,   9,  34,  35,  49,\n",
       "           71,   0,   1,   4,   0,   1,   4,   0,   1,   4,   0,   1,   3]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "          0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "          0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "          0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "          0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "          0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196, 0.0196,\n",
       "          0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "          0.1000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2000, 0.2000,\n",
       "          0.2000, 0.2000, 0.2000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]),\n",
       "  'label_indices': tensor([ 2,  5,  8, 11, 12, 13, 16, 19, 20, 21, 24, 27, 30, 33]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -0.8497, -0.3499, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -0.4540,  0.1013, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204, -1.1204]),\n",
       "  'patient_lengths': tensor([3, 3, 3, 5, 3, 5, 3, 3, 3, 3]),\n",
       "  'timestamps': tensor([705715200, 705715200, 705715200, 710553600, 710553600, 710553600,\n",
       "          641606400, 641606400, 641606400, 692928000, 692928000, 692928000,\n",
       "          699667200, 712108800, 695692800, 695692800, 695692800, 671932800,\n",
       "          671932800, 671932800, 688521600, 702345600, 658195200, 658195200,\n",
       "          658195200, 709776000, 709776000, 709776000, 651024000, 651024000,\n",
       "          651024000, 674438400, 674438400, 674438400]),\n",
       "  'token_indices': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 63, 73, 74, 75, 76,\n",
       "          77, 78, 79, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([   0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,  643.,\n",
       "          1018., 1288.,    0.,    0.,    0.,   16.,    0.,    0.,    0.,  117.,\n",
       "             0.,    0.,    0.,  452., 1077., 1106.,    0.,    0.,    0.,   88.,\n",
       "           549.,  765.,    0.,    0.,    0.]),\n",
       "  'hierarchical_tokens': tensor([  0,   2,   4,   0,   2,   3,   0,   1,   3,  14, 112,  17,   5,  54,\n",
       "           43,   8,  59,  10,  23,  46,  39,  60,  98,   7,  36,  57,  53,  48,\n",
       "           18,  80,  58,  29,   5,   0,   2,   4,  32,  16,   0,   1,   3,  68,\n",
       "           96,  24,  26,   7,   0,   1,   4,  56,  55,  16,   8,  12,   5,  41,\n",
       "           29,  52,  89, 102,  20,  24,  22,  94,  87,  40,  70,  28,  26, 108,\n",
       "            6, 109, 101,  77,  51,   7,  90,  91,  79, 105,  19,  13,  61,  27,\n",
       "           84,  33,  25,  23,   7,   9,  34,  58,   0,   1,   4,  33,  17,   5,\n",
       "           54, 116,   8,  10,  25,  23,  31,  70,   9,   6,  98,   7,  36, 115,\n",
       "           30, 107,  57,  53,  48,  18,  80, 100,  13,  21,  12,   5,  41,  29,\n",
       "           52,  89, 102,  20,  24,  22,  94,  87,  40,  70,  28,  26, 108,   6,\n",
       "          109, 101,  77,  51,   7,  90,  91,  79, 105,  19,  13,  61,  27,  84,\n",
       "           11,  38,   0,   1,   3]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          0.5000, 0.5000, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "          0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
       "          0.0500, 0.0500, 0.0500, 0.0500, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000,\n",
       "          0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "          0.2000, 1.0000, 1.0000, 1.0000, 0.2500, 0.2500, 0.2500, 0.2500, 0.0312,\n",
       "          0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
       "          0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
       "          0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
       "          0.0312, 0.0312, 0.0312, 0.0312, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429,\n",
       "          0.1429, 0.1429, 1.0000, 1.0000, 1.0000, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "          0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "          0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
       "          0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0312, 0.0312, 0.0312, 0.0312,\n",
       "          0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
       "          0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
       "          0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312, 0.0312,\n",
       "          0.0312, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000]),\n",
       "  'label_indices': tensor([ 2,  5,  8,  9, 10, 11, 14, 15, 18, 19, 22, 23, 24, 25, 28, 29, 30, 31,\n",
       "          34]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
       "          -1.1204,  1.1112,  2.4126,  3.3497, -1.1204, -1.1204, -1.1204, -1.0648,\n",
       "          -1.1204, -1.1204, -1.1204, -0.7143, -1.1204, -1.1204, -1.1204,  0.4483,\n",
       "           2.6174,  2.7180, -1.1204, -1.1204, -1.1204, -0.8150,  0.7850,  1.5346,\n",
       "          -1.1204, -1.1204, -1.1204]),\n",
       "  'patient_lengths': tensor([3, 3, 6, 4, 4, 6, 6, 3]),\n",
       "  'timestamps': tensor([666144000, 666144000, 666144000, 701481600, 701481600, 701481600,\n",
       "          692150400, 692150400, 692150400, 747705600, 780105600, 803433600,\n",
       "          690336000, 690336000, 690336000, 691718400, 675820800, 675820800,\n",
       "          675820800, 685929600, 711158400, 711158400, 711158400, 750211200,\n",
       "          804211200, 806716800, 685843200, 685843200, 685843200, 693446400,\n",
       "          733276800, 751939200, 708134400, 708134400, 708134400]),\n",
       "  'token_indices': tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  31,  33,  34,\n",
       "           35,  36,  38,  39,  40,  41,  46,  47,  48,  49,  53,  85,  92,  93,\n",
       "           94,  95, 122, 154, 156, 157, 158, 159]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True])},\n",
       " {'ages': tensor([  0.,   0.,   0., 466., 651.,   0.,   0.,   0.]),\n",
       "  'hierarchical_tokens': tensor([  0,   1,   3,  42,  10,  33,  37,  23,  48, 106,  50,   7,  92,   9,\n",
       "           34,  35,  58,  11,   8,  20,  27,   5,  18, 107,   6,  21,   0,   1,\n",
       "            4]),\n",
       "  'hierarchical_weights': tensor([1.0000, 1.0000, 1.0000, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "          0.0625, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 1.0000,\n",
       "          1.0000, 1.0000]),\n",
       "  'label_indices': tensor([2, 3, 4, 7]),\n",
       "  'normalized_ages': tensor([-1.1204, -1.1204, -1.1204,  0.4969,  1.1389, -1.1204, -1.1204, -1.1204]),\n",
       "  'patient_lengths': tensor([5, 3]),\n",
       "  'timestamps': tensor([657849600, 657849600, 657849600, 698112000, 714096000, 653097600,\n",
       "          653097600, 653097600]),\n",
       "  'token_indices': tensor([ 0,  1,  2,  3, 19, 26, 27, 28, 29]),\n",
       "  'valid_tokens': tensor([True, True, True, True, True, True, True, True])}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches['train']['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008b7853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  5,  6,  7, 10, 13, 16, 17, 20, 23, 24, 25, 26, 29, 30, 33])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_batches['train']['transformer'][0]['label_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3a93388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_batches['train']['transformer'][0]['normalized_ages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46ec51e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([189, 189, 189, 187, 187, 187, 187, 187,  63,  63,  63,  94,  94,  94,\n",
       "          64,  64,  64,  64,  75,  75,  75, 144, 144, 144, 144, 144, 144, 162,\n",
       "         162, 162, 162, 158, 158, 158]),\n",
       " tensor([125, 125, 125, 125, 125, 149, 149, 149, 114, 114, 114,  67,  67,  67,\n",
       "         199, 199, 199, 199, 199,  45,  45,  45,  23,  23,  23,  23,  23,  23,\n",
       "          23, 168, 168, 168,  59,  59,  59,  59]),\n",
       " tensor([185, 185, 185,   1,   1,   1,   1, 143, 143, 143, 117, 117, 117, 181,\n",
       "         181, 181, 181,  89,  89,  89,  27,  27,  27,  49,  49,  49, 107, 107,\n",
       "         107, 107, 150, 150, 150, 193, 193, 193]),\n",
       " tensor([128, 128, 128,  37,  37,  37,   7,   7,   7,   7,   7, 174, 174, 174,\n",
       "         169, 169, 169, 124, 124, 124,  30,  30,  30,  88,  88,  88,  88,  88,\n",
       "          40,  40,  40, 152, 152, 152]),\n",
       " tensor([122, 122, 122, 122, 122, 122,  86,  86,  86,  20,  20,  20,  57,  57,\n",
       "          57, 153, 153, 153, 192, 192, 192, 192,  83,  83,  83,  83,  83,  83,\n",
       "          83,  15,  15,  15,  15]),\n",
       " tensor([ 95,  95,  95,  95, 101, 101, 101, 101, 101,  10,  10,  10,  53,  53,\n",
       "          53,  53,  53, 126, 126, 126, 126, 157, 157, 157,  61,  61,  61, 118,\n",
       "         118, 118,  54,  54,  54,  54,  54]),\n",
       " tensor([ 29,  29,  29,  33,  33,  33,  69,  69,  69,  62,  62,  62,  62,  62,\n",
       "         135, 135, 135, 120, 120, 120, 120, 120,  80,  80,  80, 182, 182, 182,\n",
       "          96,  96,  96,  28,  28,  28]),\n",
       " tensor([183, 183, 183, 133, 133, 133,  18,  18,  18,  18,  18,  18,  42,  42,\n",
       "          42,  42,  12,  12,  12,  12, 100, 100, 100, 100, 100, 100, 160, 160,\n",
       "         160, 160, 160, 160, 136, 136, 136]),\n",
       " tensor([115, 115, 115, 115, 115,  13,  13,  13])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches['train']['patient_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3adc6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ages torch.Size([34])\n",
      "tensor([  0.,   0.,   0.,   0.,   0.,   0., 270., 436.,   0.,   0.,   0.,   0.,\n",
      "          0.,   0.,   0.,   0.,   0.,  21.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "         82., 188., 273.,   0.,   0.,   0.,  42.,   0.,   0.,   0.])\n",
      "hierarchical_tokens torch.Size([82])\n",
      "tensor([  0,   1,   3,   0,   1,   4, 118,  42,  20,  11,  95,   5,  40,  82,\n",
      "         28,   6,   0,   1,   4,   0,   1,   4,   0,   1,   4,  17,  12,   5,\n",
      "         54,  41,  52,  89,  11,   8,  42,  20,  23,  95,  47,  40,  14, 103,\n",
      "         45,  28, 108,   6, 113,  62, 101,  51,   7,  57,  93,  90,  48,  18,\n",
      "         82,  91,  79,  58,  27,  84,   0,   1,   4,   0,   1,   4,   7,  19,\n",
      "         44,  74,  16,  50,   9,   0,   2,   3, 121,   0,   1,   3])\n",
      "hierarchical_weights torch.Size([82])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.1111, 0.1111,\n",
      "        0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0270, 0.0270,\n",
      "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
      "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
      "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270,\n",
      "        0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 0.0270, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5000, 0.5000, 0.3333, 0.3333,\n",
      "        0.3333, 0.5000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n",
      "label_indices torch.Size([16])\n",
      "tensor([ 2,  5,  6,  7, 10, 13, 16, 17, 20, 23, 24, 25, 26, 29, 30, 33])\n",
      "normalized_ages torch.Size([34])\n",
      "tensor([-1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -0.1833,  0.3928,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.0475, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -0.8358, -0.4679, -0.1729, -1.1204, -1.1204, -1.1204, -0.9746, -1.1204,\n",
      "        -1.1204, -1.1204])\n",
      "patient_lengths torch.Size([9])\n",
      "tensor([3, 5, 3, 3, 4, 3, 6, 4, 3])\n",
      "timestamps torch.Size([34])\n",
      "tensor([707011200, 707011200, 707011200, 711072000, 711072000, 711072000,\n",
      "        734400000, 748742400, 653616000, 653616000, 653616000, 701308800,\n",
      "        701308800, 701308800, 709603200, 709603200, 709603200, 711417600,\n",
      "        644716800, 644716800, 644716800, 650073600, 650073600, 650073600,\n",
      "        657158400, 666316800, 673660800, 662601600, 662601600, 662601600,\n",
      "        666230400, 667094400, 667094400, 667094400])\n",
      "token_indices torch.Size([35])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
      "        62, 63, 64, 65, 66, 67, 68, 70, 73, 75, 76, 77, 78, 79, 80, 81, 82])\n",
      "valid_tokens torch.Size([34])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "for key in train_batches['train']['transformer'][0]:\n",
    "    try:\n",
    "        print(key, train_batches['train']['transformer'][0][key].shape)\n",
    "        print(train_batches['train']['transformer'][0][key])\n",
    "    except:\n",
    "        print(key, len(train_batches['train']['transformer'][0][key]))\n",
    "        print(train_batches['train']['transformer'][0][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6290a4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1300074. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1300074. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzphuo\u001b[0m (\u001b[33mstanford_som\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/pi/nigam/projects/zphuo/repos/femr/tutorials/wandb/run-20240208_042248-suz0e7op</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stanford_som/huggingface/runs/suz0e7op' target=\"_blank\">neat-donkey-33</a></strong> to <a href='https://wandb.ai/stanford_som/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stanford_som/huggingface' target=\"_blank\">https://wandb.ai/stanford_som/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stanford_som/huggingface/runs/suz0e7op' target=\"_blank\">https://wandb.ai/stanford_som/huggingface/runs/suz0e7op</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_indices tensor([ 2,  3,  4,  7, 10, 13, 16, 17, 18, 21, 24, 25, 26, 27, 28, 31, 34, 35])\n",
      "normalized_ages tensor([-1.1204, -1.1204, -1.1204, -0.9399,  0.6878, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -0.8878, -0.8219, -1.1204, -1.1204, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -0.2284,  0.5941,  1.2569,  2.2460, -1.1204, -1.1204, -1.1204,\n",
      "        -1.1204, -1.1204, -1.1204, -0.8913])\n",
      "before in norm torch.Size([36, 64])\n",
      "after in norm torch.Size([36, 64])\n",
      "pos_embed torch.Size([36, 1, 8]) torch.Size([36, 1, 8])\n",
      "x in each layer torch.Size([36, 64])\n",
      "x in each layer torch.Size([36, 64])\n",
      "final torch.Size([36, 64])\n",
      "tensor([ 2,  3,  4,  7, 10, 13, 16, 17, 18, 21, 24, 25, 26, 27, 28, 31, 34, 35])\n",
      "torch.Size([36, 64])\n",
      "features torch.Size([18, 64])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(TARGET_DIR, \u001b[39m'\u001b[39m\u001b[39mmotor_model\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/trainer.py:1561\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1560\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1561\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1562\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1563\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1564\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1565\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1566\u001b[0m     )\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/trainer.py:1895\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1892\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1894\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1895\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1897\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1898\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1900\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1901\u001b[0m ):\n\u001b[1;32m   1902\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1903\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/trainer.py:2815\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2812\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2814\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2815\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2817\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2818\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/transformers/src/transformers/trainer.py:2838\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2836\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2837\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2838\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2839\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2840\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2841\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/femr/src/femr/models/transformer.py:370\u001b[0m, in \u001b[0;36mFEMRModel.forward\u001b[0;34m(self, batch, return_loss)\u001b[0m\n\u001b[1;32m    368\u001b[0m     features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, features\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    369\u001b[0m     features \u001b[39m=\u001b[39m features[batch[\u001b[39m\"\u001b[39m\u001b[39mtransformer\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mlabel_indices\u001b[39m\u001b[39m\"\u001b[39m], :]\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtask_model(features, batch[\u001b[39m\"\u001b[39;49m\u001b[39mtask\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    371\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    372\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    373\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mpatient_ids\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    374\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mtransformer\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtimestamps\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mdatetime64[s]\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mastype(datetime\u001b[39m.\u001b[39mdatetime),\n\u001b[1;32m    375\u001b[0m         features,\n\u001b[1;32m    376\u001b[0m     )\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/miniconda3/envs/FEMR_ENV/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/share/pi/nigam/projects/zphuo/repos/femr/src/femr/models/transformer.py:279\u001b[0m, in \u001b[0;36mMOTORTaskHead.forward\u001b[0;34m(self, features, batch, return_logits)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_logits:\n\u001b[1;32m    278\u001b[0m     time_dependent_logits \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m exit()\n\u001b[1;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m loss, time_dependent_logits\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(os.path.join(TARGET_DIR, 'motor_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a41e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FEMR_ENV",
   "language": "python",
   "name": "femr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11 | packaged by conda-forge | (main, May 10 2023, 18:58:44) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd210c0e8761410aac1aa73898a7228901cf13f71a476a6a00dddfdd82855066"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
